\chapter{Multiresolution Sinusoidal Neural Networks}
\label{chap:mr_snn}

In this chapter, we present a novel neural network architecture designed to efficiently encode signals at multiple scales. Our objective is to construct a continuous, compact, and high-fidelity representation of scalar and vector fields, making it applicable to a variety of media such as audio, images, shapes, and complex spatiotemporal data. These media object, interpreted as signals, can be given explicitly as attributes of space-time coordinates or implicitly as level sets of functions.

% The core idea is to utilize coordinate-based neural networks with sinusoidal activation functions, allowing us to capture fine-grained details while maintaining a smooth global structure across different resolutions.

This chapter builds upon our exploration of sinusoidal neural networks, where we analyzed how frequency capacity and initialization schemes influence the network's ability to represent diverse signal frequencies. Drawing on these insights, we propose a multiresolution neural network (MR-Net) architecture that combines classical multi-scale signal processing techniques with modern deep learning methodologies to address the limitations of existing coordinate-based models.

The MR-Net framework introduces a unified architecture capable of encoding signals in a multiscale manner, supporting flexible training regimes and progressive refinement. It includes three primary subclasses, each designed for distinct frequency control scenarios: \emph{S-Net}, \emph{L-Net}, and \emph{M-Net}. These subclasses offer different trade-offs between frequency resolution and control, and computational efficiency, providing a versatile toolkit for representing complex signals across a range of applications.

Key features of the MR-Net framework include:

\begin{itemize}
    \item \textbf{Flexible Data Handling}: MR-Net can be trained with various types of data, including regularly sampled grids, hierarchical multiresolution structures such as pyramids, or through stochastic and stratified sampling.
    \item \textbf{Adaptive Level of Detail}: The network can adapt its representation to match the input data's resolution. For multiresolution training, levels of detail are determined by spectral projections, while for single-resolution inputs, they are governed by the network's capacity and initialization control.
    \item \textbf{Progressive Training Paradigms}: Training proceeds in a progressive manner, refining the representation incrementally. This approach ensures efficient learning across a range of scales and improves convergence for high-frequency details.
    \item \textbf{Continuous Multiscale Representation}: The MR-Net architecture encodes signals in a continuous manner, both in spatial and scale dimensions, enabling seamless reconstruction at arbitrary resolutions.
\end{itemize}

In the following sections, we describe the theoretical foundations, design principles, and architectural details of MR-Net, providing a comprehensive overview of the framework and its potential applications.


% \section{Motivation}
% \label{s-motivation}
% Let $\gt{f}:\mathcal{D}\to \mathcal{C}$ be a \textit{signal}, the \textit{ground-truth}, where $\mathcal{D}$ and $\mathcal{C}$ are finite vector spaces representing the domain and codomain of~$\gt{f}$. For instance, to represent an image, we can choose $\mathcal{D}=\R^2$ to represent the image's support and $\mathcal{C}=\R^3$ to represent the RGB \textit{color space}.
% Throughout the text, the cursive letter will indicate the ground-truth while the standard letter indicates the corresponding neural network.

% We can decompose the signal into a sum $\gt{f}=\gt{g}_0+\dots+\gt{g}_{N-1}$ of $N$ stages, where $\gt{g}_0$ is its coarsest approximation and $\gt{g}_i$, for $i>0$, progressively introduce finer details. The \textit{level of detail}~$i$ is defined as 
% $\gt{f}_i\!=\!\gt{g}_0\!+\!\cdots\!+\!\gt{g}_{i}$, or as 
% $\gt{f}_i\!=\!\gt{f}\!-\left(\gt{g}_{i+1}\!+\!\cdots\!+\!\gt{g}_{N-1}\right)$. 
% Thus, the stages can be defined as
% $$\gt{g}_i=\gt{f}_{i+1}-K*\gt{f}_{i+1}, \text{ with } \gt{f}_{N-1}=\gt{f}.$$

% That is, each stage $\gt{g}_i$ is the difference between the level $i+1$ and its convolution with a low-pass filter $K$. For example, $K$ could be a \textit{Gaussian kernel} $G(x,t)=\frac{1}{2\pi t}\exp{(-\frac{\norm{x}^2}{2t})}$, where $t$ is the scale parameter. The sequences $\{\gt{f}_i\}$ and $\{\gt{g}_i\}$ resemble the \textit{Gaussian} and \textit{Laplacian pyramids}, which are widely used in the multiresolution analysis of digital images (\cite{rosenfeld2013multiresolution,lindeberg1994scale,velho2009image}).

% We address the problem of representing a signal $\gt{f}$ in multiresolution using sinusoidal neural networks. Motivated by the decomposition $\gt{f}=\gt{g}_0+\dots+\gt{g}_{N-1}$, we consider an aggregation of $N$ sinusoidal MLPs $g_i:\mathcal{D}\to \mathcal{C}$, which we call stages, to approximate $\gt{f}$. Therefore, we propose training each network stage $g_i$ by fitting it to the stages $\gt{g}_i$ of $\gt{f}$. This approach allows us to learn the frequency of $\gt{f}$ in a controlled manner, starting from lower details and gradually moving towards higher ones.
% This is also inspired by the foundational experiments presented in Chapter \ref{chap:sinusoidal} that demonstrated that we can advantage of the MLPs spectral bias to learn the lower frequencies first using sinusoidal neural networks. We use a specific range of frequencies in the initialization of each network $g_i$ to control its represented frequenices such that we can fit it to the stages $\gt{g}_i$ of $\gt{f}$.

\section{Multiscale Representation and Learning}
% **"Multiresolution Signal Decomposition"**: Emphasizes the mathematical framework you’re presenting.
\label{s-motivation}

% ### Title Suggestions
% Rather than "Motivation," a more fitting title could be:
% - **"Multiresolution Signal Decomposition"**: Emphasizes the mathematical framework you’re presenting.
% - **"Multiscale Representation and Learning"**: Highlights the hierarchical nature of the representation.
% - **"Hierarchical Frequency Learning"**: Captures the progressive training aspect.
% - **"Structured Signal Approximation"**: Suggests a systematic breakdown of the signal.

Let $\gt{f}:\mathcal{D}\to \mathcal{C}$ be a \textit{ground-truth signal}, where $\mathcal{D}$ and $\mathcal{C}$ are finite-dimensional vector spaces representing the signal's domain and codomain, respectively. For instance, to represent an image, $\mathcal{D}$ can be set to $\R^2$ to define the spatial domain, while $\mathcal{C}=\R^3$ corresponds to the RGB color space. Throughout this text, we use a cursive font to denote ground-truth signals and a standard font for their corresponding neural network approximations.

To represent the signal $\gt{f}$ at multiple scales, we decompose it into a sum of $N$ stages: $\gt{f}=\gt{g}_0+\dots+\gt{g}_{N-1}$, where $\gt{g}_0$ captures the coarsest approximation of the signal and $\gt{g}_i$, for $i>0$, progressively introduce higher frequency components. This multiresolution decomposition enables us to define the \textit{level of detail}~$i$ in two equivalent forms:

\begin{equation}
\gt{f}_i = \gt{g}_0 + \cdots + \gt{g}_i \quad \text{or} \quad \gt{f}_i = \gt{f} - \sum_{j=i+1}^{N-1} \gt{g}_j.
\end{equation}

Each stage $\gt{g}_i$ can be computed as:
\begin{equation}
% \[
\gt{g}_i = \gt{f}_{i+1} - K * \gt{f}_{i+1}, \quad \text{where } \gt{f}_{N-1} = \gt{f}.
% \]
\end{equation}

That is, each stage $\gt{g}_i$ represents the difference between the $(i+1)$-th level and its smoothed version obtained by convolving with a low-pass filter $K$. For instance, $K$ could be a \textit{Gaussian kernel} defined as $G(x,t)=\frac{1}{2\pi t}\exp{(-\frac{\norm{x}^2}{2t})}$, where $t$ is the scale parameter. This way, the sequences $\{\gt{f}_i\}$ and $\{\gt{g}_i\}$ correspond to the \textit{Gaussian} and \textit{Laplacian pyramids}, which are standard tools in multiresolution analysis of digital images \cite{rosenfeld2013multiresolution,lindeberg1994scale,velho2009image}.

Given this multiscale decomposition, our goal is to represent a signal $\gt{f}$ using a family of sinusoidal neural networks. Motivated by the hierarchical structure of $\gt{f}=\gt{g}_0+\dots+\gt{g}_{N-1}$, we propose an aggregation of $N$ sinusoidal MLPs, denoted by $g_i:\mathcal{D}\to \mathcal{C}$, to approximate each stage $\gt{g}_i$. By training each $g_i$ to match the corresponding stage $\gt{g}_i$, we gain precise control over the frequency spectrum learned by each network. This approach leverages the spectral bias of sinusoidal networks, as demonstrated in Chapter \ref{chap:sinusoidal}, allowing the network to naturally prioritize lower frequencies and gradually introduce higher-frequency details.

Each network stage $g_i$ is initialized with a specific and crescent range of frequencies, ensuring that the resulting approximation accurately matches the desired stage $\gt{g}_i$. This structured frequency learning enables us to decompose and reconstruct complex signals in a progressive and controlled manner.

% \section{MR-Net Architecture}

% We define the MR-Net as a function $f:\mathcal{D}\times [0,N]\to \mathcal{C}$, expressed as follows:
% \begin{align}\label{e-mrnet}
% f(x,t) = c_0(t)g_0(x) + \cdots + c_{N-1}(t)g_{N-1}(x),
% \end{align}
% Each function $g_i:\mathcal{D}\to\mathcal{C}$ is a sinusoidal MLP called MR-Module (see Sec~\ref{s-mr-module} for its precise definition) and represents the $i$th \textit{stage} of $f$, whose contribution is controlled by the function 
% \begin{align}\label{e-control}
% c_i(t)=\max\Big\{0, \min\big\{1, t-i\big\}\Big\}.
% \end{align}
% Note that when $t<i$, $c_i(t)=0$; when $i\leq t\leq i+1$, $c_i(t)=t-i$; and when $t>i+1$, $c_i(t)=1$.
% Therefore, if $t=k+\delta$ with $k\in\mathbb{N}$ and $0\leq\delta\leq 1$, we obtain $$f(x,t)=g_0(x)+\dots + g_k(x)+\delta g_{k+1}(x).$$ 


% $f_t:=f(\cdot, t):\mathcal{D}\to \mathcal{C}$ is the \textit{level of detail} $t$ of the MR-Net~$f$.
% These levels evolve continuously, allowing us to encode a continuous multiresolution of $f$ at full resolution $f_N\!=\!g_0+\cdots + g_{N-1}$.


% For this, we propose fitting $f$ to the multiresolution given by the ground-truth signal $\gt{f}=\gt{g}_0+\dots+\gt{g}_{N-1}$.
% We initialize the parameters of each stage $g_i$ with sufficient capacity to fit the stage $\gt{g}_i$ (see Sec~\ref{s-frequency-control}), then train each $g_i$ to approximate $\gt{g}_i$.

% The resulting MR-Net $f$ learns the decomposition of the ground-truth signal $\gt{f}$ as a projection into the coarse scale space and a sequence of finer detail spaces. For instance, the initial stage $f_1=g_0$ provides the least detailed approximation~of~$f_N$. 
% The subsequent stages, represented by $g_i$ with $i<0$, progressively introduce finer details and are regulated by the scale parameter $t$. 
% Essentially, the multiresolution can be navigated using the scale parameter $t$ within the interval $[0,N]$, which makes this architecture closely aligned with the Multiresolution Analysis~\cite{mallat-mr89}. Fig~\ref{f:mrnet-arch} shows the structure of a MR-Net having $N$ stages. 

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.96\linewidth]{img/ch4/mr-net-stages-v2.png}
% \caption{Anatomy of the MR-Net Family.}
% \label{f:mrnet-arch}
% \end{figure}


\section{MR-Net Architecture}

The MR-Net architecture is a family of continuous multiresolution networks designed to approximate a complex signal by progressively adding details at multiple levels of resolution. The architecture is defined as a function \( f:\mathcal{D} \times [0,N] \to \mathcal{C} \), where \(\mathcal{D}\) and \(\mathcal{C}\) represent the domain and codomain, respectively. Formally, the MR-Net is expressed as follows:

\begin{align}\label{e-mrnet}
f(x,t) = c_0(t) g_0(x) + \cdots + c_{N-1}(t) g_{N-1}(x),
\end{align}

where each function \( g_i : \mathcal{D} \to \mathcal{C} \) is a \textbf{Multiresolution (MR) Module} —a building block of the MR-Net that is defined in detail in Sec~\ref{s-mr-module}. The MR-Net is composed of \( N \) such MR Modules, each corresponding to a different stage in the multiresolution hierarchy of \( f \). 

The contributions of each MR Module \( g_i \) are controlled by a set of blending functions \( c_i(t) \), defined as:

\begin{align}\label{e-control}
c_i(t) = \max \Big\{ 0, \min \big\{ 1, t - i \big\} \Big\}.
\end{align}

Intuitively, this function ensures that:

\begin{itemize}
    \item When \( t < i \), \( c_i(t) = 0 \).
    \item When \( i \leq t \leq i+1 \), \( c_i(t) = t - i \).
    \item When \( t > i + 1 \), \( c_i(t) = 1 \).
\end{itemize}

Therefore, if \( t = k + \delta \) (with \( k \in \mathbb{N} \) and \( 0 \leq \delta \leq 1 \)), we can express \( f(x, t) \) as a linear combination:

\begin{align}
f(x, t) = g_0(x) + \cdots + g_k(x) + \delta \, g_{k+1}(x).
\end{align}

Each stage function \( f_t := f(\cdot, t) : \mathcal{D} \to \mathcal{C} \) defines a \textit{level of detail} parameterized by \( t \). The resulting MR-Net $f$ learns the decomposition of the ground-truth signal $\gt{f}$ as a projection into the coarse scale space and a sequence of finer detail spaces. For instance, the initial stage $f_1=g_0$ provides the least detailed approximation~of~$f_N$. As \( t \) evolves from 0 to \( N \), these stages progressively refine the representation of the signal, with \( f_N = g_0 + \cdots + g_{N-1} \) representing the full-resolution approximation.

To train this architecture, we fit \( f \) to the ground-truth signal \( \gt{f} = \gt{g}_0 + \cdots + \gt{g}_{N-1} \) by initializing and training each stage \( g_i \) to approximate the corresponding stage of the ground-truth decomposition (see Sec~\ref{s-frequency-control} for more details on frequency control).

This structure aligns with the concept of Multiresolution Analysis (\cite{mallat-mr89}), where a function is decomposed into a sequence of approximations and detail spaces, each stage refining the approximation of the original signal. The MR-Net enables a continuous traversal through these stages using the scale parameter \(t\), allowing seamless navigation across multiple resolutions. Fig.~\ref{f:mrnet-arch} shows the general architecture of an MR-Net having \(N\) stages.

\begin{figure}[!h]
\centering
\includegraphics[width=0.96\linewidth]{img/ch4/mr-net-stages-v2.png}
\caption{Anatomy of the MR-Net Family.}
\label{f:mrnet-arch}
\end{figure}


% ## MR Module
% \label{s-mr-module}

% Each stage \( g_i \) in the MR-Net is a **Multiresolution (MR) Module**, defined as a sinusoidal MLP (Multilayer Perceptron) with induced frequency bands. An MR Module is constructed as a function \( g_i = L_i \circ H_i \circ S_i \), where:

% - **\( S_i \)**: The sinusoidal input layer that projects \( x \in \mathcal{D} \) into a list of sines, generating \( S_i(x) \).
% - **\( H_i \)**: A sequence of hidden layers (optional), composing \( k \) intermediate transformations of the sines.
% - **\( L_i \)**: The final linear layer that combines these transformations.

% The specific choice and configuration of these layers define the two types of MR Modules:

% 1. **Pure Sine MR-Module**: A shallow MLP with only the sinusoidal layer \( S \) and linear layer \( L \).
% 2. **Modulated Sine MR-Module**: A deep MLP that includes one or more hidden layers \( H \).

% The control layer \( c_i(t) \) regulates the blending of the MR Modules at each level, allowing for continuous transitions between stages.

% ### 1. Pure Sine MR-Module
% The Pure Sine MR-Module is a basic building block defined as \( g_i = L \circ S \). The sinusoidal layer \( S \) has the form:

% \[
% S(x) = \sin(W_s x + b_s),
% \]

% where \( W_s \in \mathbb{R}^{m \times d} \) and \( b_s \in \mathbb{R}^m \) are the weights and biases, respectively, and \( m \) is the number of sine functions. The linear layer \( L \) maps the output of \( S \) to a scalar value:

% \[
% L(y) = W_l y + b_l, \quad W_l \in \mathbb{R}^{1 \times m}, \, b_l \in \mathbb{R}.
% \]

% This architecture acts as a spectral filter that selects specific frequencies from the input signal based on the initialization of \( W_s \).

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.8\linewidth]{img/ch4/pure-sine.png}
% \caption{Pure sine MR-Module.}
% \label{f:pure-sine}
% \end{figure}

% ### 2. Modulated Sine MR-Module
% The Modulated Sine MR-Module is a deeper variant defined as \( g_i = L \circ H \circ S \). The hidden layers in \( H \) consist of multiple sinusoidal transformations:

% \[
% H(x) = \sin(W x + b),
% \]

% where each transformation introduces nonlinearity and enhances the network’s expressivity. This depth allows the network to capture more complex signal patterns.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.9\linewidth]{img/ch4/modulated-sine.png}
% \caption{Modulated sine MR-Module.}
% \label{f:modulated}
% \end{figure}

% With these two types of modules, the MR-Net can effectively model signals with varying levels of detail, navigating through its multiresolution hierarchy using the scale parameter \( t \).


\subsection{MR Module}
\label{s-mr-module}
Each stage $g_i$ of the MR-Net $f$ is a sinusoidal MLP, called \textit{MR-Module}, that learns a signal representation as a combination of sinusoidal functions with induced frequency band.

We write each stage as $g_i=L_i\circ H_i\circ S_i$. The first layer projects the input $x$ into a list of sines $S_i(x)$, which is the input of the composition $H_i$ of the MLP hidden layers. The output $H_i\circ S_i(x)$ is a dictionary of sine combinations that are passed to the linear layer $L_i$. Finally, the control layer $c_i(t)$, with $t\in[i,i+1]$, blends the stage $g_i$ with the level of detail $f(x,i-1)$. 

There are two kinds of MR-Modules: \emph{Pure sine} MR-Module ($H_i=\emptyset$) and \emph{modulated sine} MR-Module ($H_i\neq\emptyset$). In the first case the network has only the first layer and the linear layer, and in the second case the network has the three blocks, including the hidden layers (see Fig~\ref{f:mr-module}).

\begin{figure}[!h]
\centering
% \includesvg[width=\linewidth]{svg_figs/diagram_mrnet.svg}
\includegraphics[width=\linewidth]{img/ch4/diagram_mr_module.pdf}
\caption{General Anatomy of a MR-Module.}
\label{f:mr-module}
\end{figure}

Without loss of generality, let us assume that the ground-truth signal is a grayscale image unless stated to the contrary. In this case, the stages of the MR-Net will have $\R^2$ and $\R$ as domain and codomain respectively. 
Consequently, a MR-Net with $N$ stages $g_i:\R^2\to \R$ has the form $f:\R^2\times [0,N]\to \R$.
Under this assumption, we present the building blocks of the MR-Net with details. 


\subsubsection{Pure Sine MR-Module}

The pure sine MR-Module is a shallow sinusoidal MLP defined as the composition $L\circ S$ of a sinusoidal layer $S$ with a linear layer $L$.
The layer $S\!:\!\R^2\!\to\! \R^m$ projects the input $x$ into a dictionary of $m$ sines of the form $S(x)=\sin\left(W_s x+ b_s\right)$, where $W_s\in \R^{m\times 2}$ and $b_s\in \R^{m}$ are the \textit{weight matrix} and \textit{bias}. The integer $m$ is the \textit{width} of the MR-Module.
The layer $L\!:\!\R^m\!\to\! \R$ is an affine map $L(x)=W_lx+b_l$, where $W_l\in \R^{1\times m} $ and $b_l\in \R$  are the final weight matrix and bias.
Hence, $L\circ S$ acts as a spectral filter controlled by the initialization of $W_s$.


Since the risk landscape for the network loss has many local minima, it is expected that the weights $W_s$ can’t move much further than their initialization. Therefore, $W_s$ determine the range of frequencies one can filter from the input signal. Fig~\ref{f:pure-sine} shows the structure of the $L\circ S$ and an example of a signal consisting of a linear combination of two frequencies. In this example, we have the layers $S\!:\!\R\!\to\! \R^2, S(x) = (sin(x), sin(5x))$ and $L\!:\!\R^2\!\to\! \R, L(x_1, x_2) = x_1 + x_2$.

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{img/ch4/pure-sine.png}
\caption{Pure sine MR-Module.}
\label{f:pure-sine}
\end{figure}


\subsection{Modulated sine MR-Module}

The Modulated MR-Module is a deep sinusoidal MLP $L\circ H\circ S$ that considers a hidden sinusoidal layer block $H:\R^m\to~\R^m$. 
This map is the composition of $k$ layers $H=H_k\circ \dots \circ H_1$ with $H_i(x_i)\!=\!\sin\left(W_i x_i \!+\! b_i\right)\!=\!x_{i+1}$ where $x_0\!\in\!\R^m$ is the input.
The integers $m$ and $k+1$ are the \textit{width} and \textit{depth} of the MR-Module.  

The MR-Module $L\circ H\circ S$ is a linear combination of a dictionary of (spectral) atoms $H\circ S$ containing composition of sines. This fact has two consequences. First, the capacity of $L\circ H\circ S$ is greater than the capacity of $L\circ S$ with the same number of neurons~\cite{novello2022understanding}. Second, the initialization of $L\circ H\circ S$ does not directly control its frequency band as in the case of $L\circ S$.

In terms of localization properties in space and frequency, we can say the spectral atoms of the Modulated MR-Module are semi-local in the sense that these functions are adapted by the learning process to fit local variations of the signal (i.e., frequencies) across its (spatial) domain. This characteristic is evident in Fig~\ref{f:modulated} that shows a network structure and the graph of $\sin\Big(3\sin\big(5\sin(1.9x)\big)\Big)$.
In this example, the first layer is represented by $S(x)=\sin(1.9 x)$, the hidden block is the composition of two sine layers $H(x)=\sin\big(3\sin(5x)\big)$, and the linear layer is the identity function $L(x)=x$.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{img/ch4/modulated-sine.png}
\caption{Modulated sine MR-Module.}
\label{f:modulated}
\end{figure}



\section{MR-Net Subclasses}

Here, we describe three subclasses of the MR-Net: \emph{S-Net}, \emph{L-Net}, and \emph{M-Net}.
The characteristics of their level of details depend on the configuration of their stages. %, as will be presented next.
Recall, from Eq~\ref{e-mrnet}, that a MR-Net is a function $f:\R^2\times [0,N]\to \R$, expressed as:
\begin{align*}
f(x,t) = c_0(t)g_0(x) + \cdots + c_{N-1}(t)g_{N-1}(x).
\end{align*}
Therefore, to define the subclasses S-Net, L-Net, and M-Net, we only need to define the stages $g_i$.


\subsection{S-Net}

A S-Net is a MR-Net in which each stage $g_i$ is a pure sine MR-Module, i.e. $g_i=L_i\circ S_i$ where $S_i$ and $L_i$ are the first and the linear layers (see Fig~\ref{f:s-net}).
%
For this reason, each stage $g_i$ can be  
% consists of a learned ``sine transform'', with its layers corresponding, respectively, to the direct and inverse sine transform. Specifically, 
expressed as a sum of sine functions
$$g_i(x)=a_0+\sum_{j=1}^m a_j \sin\left(\omega_j x+\varphi_j\right),$$ where the \textit{frequencies} $\omega_j$ and \textit{phase-shifts} $\varphi_j$ are given by the weights and bias of the first layer $S_i$. The \textit{amplitudes} $a_j$ are given by the linear layer $L_i$.
%
As a consequence, the resulting S-Net $f$ can provide level of detail based on the controlled initialization of frequency band of each stage $g_i$ (see Sec~\ref{s-frequency-control}).
\begin{figure}[!h]
\centering
\includegraphics[width=0.58\linewidth]{img/ch4/snet.pdf}
\caption{S-Net architecture.}
\label{f:s-net}
\end{figure}

\subsection{L-Net}
\label{s-lnet}
A L-Net is a MR-Net $f$ in which each stage $g_i=L_i\circ H_i\circ S_i$ is an independent modulated sine MR-Module: $S_i$, $H_i$, $L_i$ are the first, hidden, and linear blocks (see Fig~\ref{f:l-net}).
%
For this reason, the level of detail capacity of $f$ is determined by the width and depth of each stage $g_i$.
% The first stage $g_0$ is the coarsest approximation of the underlying function.
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{img/ch4/lnet.pdf}
% \vspace{-0.4cm}
\caption{L-Net architecture. Note that the stages are independent of each other.}
\label{f:l-net}
\end{figure}


The $N$th level $f(\cdot, N)$ of a L-Net is an example of a sinusoidal MLP.
Without loss of generality, let $f(\cdot, t)=~c_0(t)g_0+c_1(t)g_1$ be a L-Net with two stages having the same architecture with a single hidden layer. We show that $f(\cdot, 2)\!=\!L\!\circ\! H\!\circ\! S$.
For this, define the matrices of $S$, $H$, and $L$, respectively, using $W_s\!=\!\begin{psmallmatrix}W_s^0\\W_s^1\end{psmallmatrix}$, 
$W_h\!=\!\begin{psmallmatrix}W_h^0& 0\\0&W_h^1\end{psmallmatrix}$, 
$W_l\!=\!\begin{psmallmatrix}W_l^0 & W_l^1\end{psmallmatrix}$, where $W_s^j$, $W_h^j$, $W_l^j$ are the matrices of the stages~$g_j$. The biases are defined in a similar way. Such procedure can be extended to a sum of $N$ stages in a analogous way.
Thus, the L-Net gives us a controllable way of increasing the \textit{width} of a sinuoisal MLP during training.


\subsection{M-Net}
\label{s-mnet}
While the stages of S-Nets and L-Nets are independent, with M-Net we propose a way to reuse information learned at previous stages.
A M-Net is a MR-Net where each stage $g_i$ is a modulated MR-Module linked to its subsequent stage $g_{i+1}$.
That~is, $H_i\circ  S_i$ is composed both with the linear layer $L_i$, producing the stage $g_i\! =\! L_i\!\circ\! H_i\!\circ\!  S_i$, and the hidden block $H_{i+1}$ resulting in $g_{i+1}\! =\! L_{i+1}\!\circ H_{i+1}\!\circ\left(S_{i+1}, H_i\circ  S_i\right)$ (Fig~\ref{f:m-net}).
This way, the hidden block has the form $H_{i+1}\!:\!\R^{2m}\!\to\! \R^m$; $m$ is the number of neurons.
\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{img/ch4/mnet.pdf}
% \vspace{-0.4cm}
\caption{M-Net architecture. Notice how the hidden-layers blocks of adjacent MR-Modules are connected.}
\label{f:m-net}
\end{figure}

The hidden block of a stage of the M-Net is composed~with~a sequence of hidden blocks coming from previous stages.~That~is, the M-Net contains a deep MLP $L_{N-1} \circ \overline{H_{N-1}} \circ \cdots \circ \overline{H_1}\circ H_0 \circ S_0$ with $N$ hidden blocks; $\overline{H_i}$ is the part of $H_i$ connecting to the stage $g_{i+1}$.
Thus, the M-Net also allows us to increase the \textit{depth} of a sinusoidal MLP during training implying that the capacity of each stage increases with its depth in the hierarchy.
This feature makes the M-Net a suitable and compact architecture for  multiresolution training, as discussed in Sec~\ref{sec:considerations}, and we will utilize this subclass in the applications of Sec~\ref{s:img}.


\subsection{Frequency Control}
\label{s-frequency-control}

Let $\gt{f}=\gt{g}_0+\cdots+\gt{g}_{N-1}$ be the ground-truth signal in multiresolution, and $f:\R^2\times [0,N]\to \R$ be a MR-Net with $N$ stages. To train each stage $g_i$, we propose to initiate its parameters based on the frequency content of $\gt{g}_i$.
There are two ways to control the frequency band of $g_i$. First, we can initialize the weight matrix of the first layer of $g_i$. Second, we can vary its width and depth. Moreover, these mechanisms can be combined.


\subsubsection{Frequency Initialization}
\label{s-frequency-initialization}


Training sinusoidal MLPs can be challenging  as periodic activation functions may lead to instability in deep architectures~\cite{taming2017}. \citet{sitzmann2019siren} propose an initialization scheme that guarantee stability during training. 
They initialize the weights $W=\omega \, \tilde{W}$ of the MLP first layer $ \sin\left(Wx + b\right)$ such that $\tilde{W}$ is uniformly sampled in $[-1,1]$ and the number $\omega$ controls the range of frequencies. That is, the layer projects the input $x$ in a list of sines with frequencies in $[-\omega, \omega]$. 
They empirically choose $\omega=30$ in their experiments. For the hidden layers, they choose the weights distributed uniformly in $\left(-\sqrt{6/m}, \sqrt{6/m}\right)$, where $m$ is the width of the layer. See \cite{sitzmann2019siren} for the details.

Regarding the initialization of the MR-Net $f$, let $g_i=L_i\circ H_i\circ S_i$ be its $i$th stage, where $S_i$, $H_i$, and $L_i$ are its first, hidden, and linear blocks.
Observe that each coordinate of the sinusoidal layer $S_i(x)=\sin\left(W_{s_i} x+b_{s_i}\right)$ has the form $\sin(\omega_1 x_1 +\omega_2 x_2 + \varphi)$, where the \textit{frequencies} $\omega_1$ and $\omega_2$ form a line of the matrix $W_{s_i}$, $x=(x_1,x_2)$ is the input, and the \textit{phase-shift} $ \varphi$ is a coordinate of the bias $b_{s_i}$. We follow the above initialization approach to initialize $g_{i}$. However, instead of using $\omega=30$ we consider it to be a \textit{bandlimit frequency} on the ground-truth stage $\gt{g}_i$.

% The initialization of the elements of the matrix $W_{s_i}$ in the sinusoidal layer $S_i(x)=\sin\left(W_{s_i} x+b_{s_i}\right)$ is of central importance in the training convergence of $f$.

Ideally, the initialization of frequencies in $g_i$ should match the frequency content of the ground-truth signal at the stage $\gt{g}_i$. 
However, as we usually don't have access to this information, we opt to use an upper bound. 
Specifically, we assume that $\gt{g}_i$ has no frequency higher than a \textit{bandlimit} $B_i$. The Nyquist–Shannon sampling theorem says that we can reconstruct $\gt{g}_i$ from a sample $\{\gt{g}_i(x_{kl})\}$, where the regular grid $x_{kl}$ has points spaced with size $\frac{1}{r_i}<\frac{1}{2B_i}$. The number $r_i$ is the \textit{sample rate}. Fig~\ref{f:nyquist} illustrates such requirement using the frequency rate notation.  
% Assuming that the signal has been properly sampled, the ``Nyquist limit'' gives the maximum frequency contained in the signal, which is $\frac{1}{2} f_s$, where $f_s$ is the sampling rate (see Fig~\ref{f:niquist}).
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{img/ch4/nyquist.png}
\vspace{-0.3cm}
\caption{Nyquist Limit.}
\label{f:nyquist}
\end{figure}


Therefore, to fit the stage $g_i$ to the sample $\{\gt{g}_i(x_{kl})\}$, we opt to initialize $g_i$ such that it can represent frequencies up to $\omega_i:=\frac{r_i}{2}$.
For this, we initialize the lines of the first matrix $W_{s_i}$ of $g_i$ uniformly in the set $\Omega_i=\left[-\omega_i, \omega_i\right]^2$.
Thus, the sinusoidal layer $S_i$ may contain frequencies already initialized at previous stages because $\{\gt{g}_i\}$ is a Laplacian pyramid of $\gt{f}$, thus, $\Omega_0\subset \cdots \subset \Omega_{N-1}$. The weights of the hidden block $H_i$ are initialized following the same scheme in \cite{sitzmann2019siren}.

% \vspace{0.2cm}
% {\color{red}
% In practice, we assume that the data is given as a regular sample (digital image) of size $2^{k}\times 2^k$
% of the ground-truth signal~$\gt{f}$.
% We abuse the notation by denoting this digital image by~$\gt{f}$.
% To train the MR-Net stages~$\{g_i\}$, we use the (discrete) \textit{Gaussian} and \textit{Laplacian} pyramids of $\gt{f}$, both with $N<k$ levels.
% Precisely, the Gaussian pyramid $\{\gt{f}_i\}$ is defined recursively by convolving the \textit{level of detail} $\gt{f}_i$ with a Gaussian kernel $K$ and downsampling the result by a factor 2:
% \begin{align*}
%     \gt{f}_i(k,l)&=\left(K*\gt{f}_{i+1}\right)(2k,2l)\,\, \text{with} \,\, k,l\in \left\{1,\ldots, 2^{k-N+1+i}\right\},\\
%     \gt{f}_{N-1}&=\gt{f}.
% \end{align*}
% $\gt{f}_i(k,l)$ denotes the digital image $\gt{f}_i$ evaluated at the pixel $(k,l)$.
% Similarly, the Laplacian pyramid $\{\gt{g}_i\}$ is defined using
% \begin{align*}
%     \gt{g}_i(k,l)&=\left(\gt{f}_{i}-K*\gt{f}_{i}\right)(2k,2l)\,\, \text{with} \,\, k,l\in \left\{1,\ldots, 2^{k-N+1+i}\right\}\\ \gt{g}_0(k,l)&=\left(K*\gt{f}_{1}\right)(2k,2l).
% \end{align*}
% Since $2^{k-N+1+i}$ is the height and width of $\gt{g}_i$, it cannot contain frequencies higher than $\omega_i=2^{k-N+i}$. 
% Thus we propose to initialize the frequencies of the $N$ stages $\{g_i\}$ of the MR-Net $f$ following a dyadic sequence of frequency bands 
% \begin{align}
% \omega_{N-1}, \omega_{N-2}, \ldots, \omega_{0}.
% \end{align}
% Which is equivalent to $\omega_{N-1}, \frac{\omega_{N-1}}{2}, \ldots, \frac{\omega_{N-1}}{2^{N-1}}$ with $\omega_{N-1}= 2^{k-1}$.
% These are the bandlimits used to define the sets $\Omega_i=\left[-\omega_i, \omega_i\right]^2$ to initialize the frequencies of the first layer of each stage $g_i$.
% (see Fig~\ref{f:freq-bands}).

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.98\linewidth]{figs/freq-bands.png}
% \caption{Frequency Bands}
% \label{f:freq-bands}
% \end{figure}

% }

% Finally, the training of each stage $g_i$ is done by minimizing the following loss functional
% \begin{align}
%     \mathscr{L}(\theta_i)=\sum_{k=1}^{2\omega_i}\sum_{l=1}^{2\omega_i}\Big(\gt{g}_i(k ,l)-g_i(k,l)\Big)^2
% \end{align}

\subsubsection{MR-Module Capacity}
\label{sec-module-capacity}

The capacity of each MR-Net stage $g_i=L_i\circ H_i\circ S_i$ is controlled by its width $m$ and depth $k+1$.
The width $m$ determines that an input $x$ will be transformed into a list of $m$ sines $S_i(x)=\sin\left(W_{s_i}x+b_{s_i}\right)$,
where $W_{s_i}$ has rows sampled at the set $\Omega_i$ defined in Section~\ref{s-frequency-initialization}. By increasing $m$, the dictionary of input frequencies is augmented. The hidden block $H_i$ consisting of $k$ sinusoidal layers, further enhances this list of frequencies.


To mitigate the effects of the spectral bias, we employ the above frequency initialization approach to the stage $g_i$. This ensures that the training of $g_i$ begins with frequencies that are \textit{close} to those present in the corresponding ground-truth data $\gt{g}_i$.
% See Fig~\ref{f:capacity}.
% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.97\linewidth]{figs/width.png}\\
% \includegraphics[width=0.97\linewidth]{figs/depth.png}
% \vspace{-0.2cm}
% \caption{Capacity Hyperparameters (from~\cite{Rahaman2018O})}
% \label{f:capacity}
% \end{figure}

Furthermore, the MR-Net architecture enables us to gradually increase its width and depth by adding stages (see Secs~\ref{s-lnet} and \ref{s-mnet}), providing us with a controlled way of increasing network capacity while allocating ground-truth frequencies across stages in a controllable manner.
For example, a L-Net can be viewed as a specific MLP where adding a new stage increases its width. As a result, L-Net allows us to divide a given MLP into stages. On the other hand, M-Net has a more sophisticated structure, as adding a stage results in a network that is wider and deeper.


% \section{MR-Net in Detail}
% \label{sec:mrnet_detail}

This section presents the MR-Net framework in detail by conceptually dividing it in four main components: \emph{MR-Structure}, \emph{MR-Stages}, \emph{MR-Training}, and \emph{MR-Inference} (see Fig~\ref{f:components}). 

% Although coordinate-based networks could be applied to represent signals in other dimensions, as demonstrated by SIREN \cite{sitzmann2019siren} and BACON \cite{bacon2021}, we are going to focus on images, that is signals XXXXX, the applications described in section XXXXX. For 1D applications, please refer to XXXXX.

\begin{figure}[!h]
\centering
\includegraphics[width=0.99\linewidth]{img/ch4/mr-net-components.png}
\caption{MR-Net framework Components.}
\label{f:components}
\end{figure}

% Let $\gt{f}:\mathcal{D}\to \mathcal{C}$ be the ground-truth signal (\textit{input data}), and $f\!:\!\mathcal{D}\!\times\! [0,N]\!\to\! \mathcal{C}$ be a MR-Net with $N$ stages $\{g_i\}$ to fit $\gt{f}$~in multiresolution.
% We use this setting to present the framework.

\subsection{MR-Structure}\label{sec:mr_struct}

The MR-Structure is a data structure that encapsulates the input data $\gt{f}$. It includes $\gt{f}$ and metadata about its \textit{sampling mode}, \textit{filtering type}, and the \textit{multi-stage stack} (see Fig~\ref{f:structure}).
\begin{figure}[!h]
\centering
\includegraphics[width=0.89\linewidth]{img/ch4/mr-structure.png}
\caption{MR-Structure.}
\label{f:structure}
\end{figure}

% \subsubsection{Input Data}

The training of MR-Net stage $g_i$ receives pairs $\{x_j, y_j\}$ as input, with points $x_j$ sampled in the domain $\mathcal{D}$ of $\gt{f}$ and $y_j=\gt{f}(x_j)\in~\mathcal{C}$. For a squared image, we could have $\mathcal{D}=[-1, 1]^2 \subset \R^{2}$. During training, we consider the signal $\gt{f}$ to have codomain in monochromatic ($\mathcal{C}=\R$) or RGB color ($\mathcal{C}=\R^3$) (see Fig~\ref{f:values}). 
Depending on the application, other color spaces or additional attributes, such as masks and features, could be used (see Fig~\ref{f:attributes}). 
\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{img/ch4/attrib-d0-d1.png}
\caption{Image values (RGB and monochromatic).}
\label{f:values}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{img/ch4/attrib-mask.png}
\caption{Examples of image attributes (mask and edges).}
\label{f:attributes}
\end{figure}


The data input for the training of each of the $N$ stages $g_i$ is organized in a \textit{multi-stage stack} $\{x_j, y_j\}_i$.
% consisting of a hierarchy of sampling grids. 
Specifically, for each $i$, the set of pairs $\{x_j, y_j\}_i$ is a sample of the $i$th level of detail $\gt{f}_i$ of the signal $\gt{f}$ or a sample of the original signal $\gt{f}$, with $\{x_j\}_i\subset \mathcal{D}$ and $\{y_j\}_i=\{\gt{f}_i(x_j)\}$. 
From the point of view of \textit{representation theory}, we can interpret this as a projection of the function $\gt{f}_i$ onto the primal \textit{Shannon basis} (i.e., Dirac delta distribution). In the context of signal processing, this basis is a sampling grid of impulses and the representation consists of the sequence $\{y_j\}_i$ at the grid locations $\{x_j\}_i$.

The type of sampling used to extract the multi-stage stack $\{x_j, y_j\}_i$ from the signal $\gt{f}$ is an important aspect in the MR-Net training. In that respect, it is instrumental to consider two types of samplings: \textit{regular} and \textit{irregular} (Fig~\ref{f:sampling}). 
%
In the regular case, the sampled points are organized in a regular grid discretization $\{x_{k,l}\}$ of the domain $\mathcal{D}$. 

% The irregular sampling consider the points $\{x_i\}$ to be uniformly sampled from $\mathcal{D}$.
% , i.e. for each pair $k,l$ we have $\norm{x_{k,l}-x_{k+1,l}}=\norm{x_{k,l}-x_{k,l+1}}=\Delta$ with $\Delta$ being a constant. 
%
For flexibility, we implement a sampler module that take regular samples or stochastic samples using the Poisson disk sampling~\cite{stochastic_cook, poisson_bridson}. 
The sampler could be extended to work with different sampling patterns. 
The sampling mode is stored in the MR-Structure, so it can be taken into account when doing operations that modify the sampling grid.

\begin{figure}[!h]
\centering
\includegraphics[width=0.45\linewidth]{img/ch4/regular.png}
\hfil
\includegraphics[width=0.45\linewidth]{img/ch4/poisson.png}
\\
{\small\hfil (regular grid) \hfil\hfil\hfil (irregular grid) \hfil}
\vspace{-0.2cm}
\caption{Sampling modes.}
\label{f:sampling}
\end{figure}


The multi-stage stack of the input signal $\gt{f}$ could have different resolutions.
For the regular case where the sampled points are organized in a regular grid $\{x_{k,l}\}$ (the highest resolution), we can structure the multi-stages in dyadic lattice following the $2^i$ rule: 
\begin{align*}
\{x_{k,l}\}_i=\{x_{2k,2l}\}_{i+1}\text{ with }\{x_{k,l}\}_N=\{x_{k,l}\}.
\end{align*}
Here we are assuming that $\{x_{k,l}\}$ is a grid of size $2^k\times 2^k$ for some integer $k>N$.
Thus each dimension of a stage grid $\{x_{k,l}\}_i$ has twice the size of the previous one (Fig~\ref{f:multi}(a)). 
On the other hand, we could consider that the sampling grids $\{x_{k,l}\}_i$ have a fixed resolution, and sample the signal $\gt{f}$ on its highest level or consider the level of detail $\gt{f}_i$ for each grid stage $\{x_{k,l}\}_i$ (Fig~\ref{f:multi}(b)). See Section~\ref{s:lod} for more details.
% Nonetheless, it is also possible to define a irregular multiresolution grid structure. In this case, each resolution level has approximately twice the number of random sample points of the previous level .
\begin{figure}[!h]
\centering
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=\linewidth]{img/ch4/pyramid.png}
\caption{Pyramid}
\end{subfigure}
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=\linewidth]{img/ch4/tower.png}    
\caption{Tower}
\end{subfigure}
\caption{Examples of multi-stage stacks.}
\label{f:multi}
\end{figure}

Each level $i$ of the multi-stage stack can be filtered to separate the level of detail $\gt{f}_{i+1}$ into different frequency bands. In this sense, we can use the unfiltered signal $\gt{f}$, a low-pass version of $\gt{f}_{i+1}$ or a band-pass version of $\gt{f}_{i+1}$. For this, it is common to employ a Gaussian kernel as the low-pass kernel and a difference of Gaussians as the band-pass kernel (see Fig~\ref{f:filter}).
Precisely, we can filter $\gt{f}_{i+1}$ by convolving it with a Gaussian kernel $K$:
% and downsampling the result by a factor 2:
\begin{align}\label{e-gaussian-filter}
    \gt{f}_i(k,l)&=\left(K*\gt{f}_{i+1}\right)(k,l)
\end{align}
We abuse the notation and denote by $\gt{f}_i(k,l)$ the function $\gt{f}_i$ evaluated at  $x_{k,l}$.
Similarly, the $i$th band-pass stage $\gt{g}_i=\gt{f}_i-\gt{f}_{i-1}$ is defined using $\gt{g}_i(k,l)=\left(\gt{f}_{i}-K*\gt{f}_{i}\right)(k,l)$.

\begin{figure}[!h]
\centering
\includegraphics[width=0.32\linewidth]{img/ch4/signal.png}
\includegraphics[width=0.32\linewidth]{img/ch4/gaussian.png}
\includegraphics[width=0.32\linewidth]{img/ch4/laplacian.png}\\
{\hfil \hfil signal \hfil \hfil \hfil low-pass \hfil \hfil \hfil band-pass \hfil}
\caption{Examples of filter types.}
\label{f:filter}
\end{figure}


% % \subsubsection{Sampling}
% % The input for the neural network is a discrete representation. In this sense,} sampling is a way to create a representation of a function based on its values at certain points of the domain. From the point of view of Representation Theory, we can interpret this as a projection of the function onto the primal Shannon basis (i.e., Dirac delta distribution). In the context of signal processing, this basis is a sampling grid of impulses and the representation consists of the sequence of signal values at the grid locations.

% % One important aspect of sampling for learning signals with coordinate-based neural network is the structure of the sampling grid. In that respect, it is instrumental to consider two types of grid structures: Regular and Irregular. See Fig~\ref{f:sampling}. {\color{red}The sampling mode is stored in the MR-Structure, so it can be taken into account when doing operations that modify the sampling grid.}

% % \begin{figure}[!h]
% % \centering
% % \includegraphics[width=0.42\linewidth]{img/ch4/regular.png}
% % \hfil
% % \includegraphics[width=0.42\linewidth]{img/ch4/poisson.png}
% % \\
% % {\hfil (regular grid) \hfil\hfil (irregular grid) \hfil}
% % \caption{Sampling Modes}
% % \label{f:sampling}
% % \end{figure}


% % \subsubsection{Filtering}

% % The input of signal values to the network can be filtered to separate it into different frequency bands. In this sense, we can use the unfiltered signal, a low-pass version of the signal or a band-pass version of the signal. See Fig~\ref{f:filter}.

% % It is common to employ a Gaussian Kernel as the low-pass Kernel and a Difference of Gaussians as the band-pass kernel.

% % \begin{figure}[!h]
% % \centering
% % \includegraphics[width=0.30\linewidth]{img/ch4/signal.png}
% % \includegraphics[width=0.30\linewidth]{img/ch4/gaussian.png}
% % \includegraphics[width=0.30\linewidth]{img/ch4/laplacian.png}\\
% % {\hfil \hfil signal \hfil \hfil \hfil low-pass \hfil \hfil \hfil band-pass \hfil}
% % \caption{Filter Types}
% % \label{f:filter}
% % \end{figure}

% % \subsubsection{multi-stage stack}

% % A Multiresolution Stack consists of a hierarchy of sampling grids with different resolutions. The standard grid structure form a dyadic lattice of regular grids obeying the $2^j$ rule, i.e., each level of the grid has twice the size of the previous one. 
% % Nonetheless, it is also possible to define a irregular multiresolution grid structure. In this case, each resolution level has approximately twice the number of random sample points of the previous level (see Fig~\ref{f:multi}(b)).
% % On the other hand, it is possible to have a stack of sampling grid with the same resolution, in which the signal is the same at each level or is filtered (see Fig~\ref{f:multi}(a)). Section~\ref{s:lod} gives more details on this option.
% % \begin{figure}[!h]
% % \centering
% % \includegraphics[width=0.45\linewidth]{img/ch4/tower.png}
% % \includegraphics[width=0.45\linewidth]{img/ch4/pyramid.png}\\
% % {\hfil tower \hfil \hfil \hfil pyramid \hfil}
% % \vspace{-0.2cm}
% % \caption{\hl{Multiresolution Hierarchies}}
% % \label{f:multi}
% % \end{figure}


\subsection{MR-Stages}\label{sec:mr_stages}

The MR-Stages $\{g_i\}$ are the build blocks of the MR-Net $f$. They constitute a stack of MR-Modules that are interconnected according to the chosen MR-Net subclass as illustrated on Figures ~\ref{f:s-net}, \ref{f:l-net}, and \ref{f:m-net}.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.3\linewidth]{img/ch4/mr-stages.png}
% \caption{MR-Stages}
% \label{f:stages}
% \end{figure}

The MR-Net configuration is given by the 
parameters of the MR-Stages: number of stages, depth and width of the stages.


\subsection{MR-Training}\label{sec:mr_training}
\label{s:training}

The training of the MR-Net $f$ must consider the mechanisms for learning different levels of detail at each stage $g_i$ of $f$. 
The capacity of $g_i$ to represent the underlying ground-truth stage  $\gt{g}_i$ can be achieved by initializing its first layer or adjusting its width and depth, as explained in Section~\ref{s-frequency-control}
.
Regarding the network input, we can either use the original signal, or pre-process the signal with a low-pass filter.


The MR-Training incorporates a \textit{loss functional}, a \textit{logger} to monitor the training, as well as, a \textit{scheduler} (see Fig~\ref{f:training}). We describe each of these components, as well as the multiresolution regime in the following sections. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.89\linewidth]{img/ch4/training.png}
\caption{MR-Training.}
\label{f:training}
\end{figure}

\subsubsection{Loss Functional}

Signal reconstruction is related to interpolation and fitting. We would like to fit the MR-Net $f$ to the ground-truth signal $\gt{f}$ using a given sample of $\gt{f}$.
For this, we observe that training $f$ is related to a \textit{regression} in multi-scale. This means that: i) the approximation should take into account a multiresolution $\gt{f}=\gt{g}_0+\cdots+\gt{g}_{N-1}$ of the ground-truth signal; and ii) each MR-Net stage $g_i$ should fit to a sample of $\gt{g}_i$.

To accomplish these goals above, we define a \emph{loss functional} $\mathcal{L}_i$ to train each stage $g_i$ of the MR-Net $f$. For this, we assume that the signal $\gt{f}$ was sampled and organized in a \textit{multi-stage stack} $\{x_j, y_j\}_i$ such that $y_j=\gt{g}_i(x_j)$.
 Thus $\mathcal{L}_i$ is defined by minimizing the differences between the true values $y_j$ at the sample points $x_j$ and the predicted values $g(x_j)$ at the $i$th stage: 
\begin{align}\label{e-loss}
    \mathcal{L}_i(\theta_i)=\frac{1}{K_i}\sum \norm{g_i(x_j)-y_j}^2.
\end{align}
Where $\theta_i$ are the parameters of $g_i$ and $K_i$ is the size of $\{x_j, y_j\}_i$.

When the multi-stage stack $\{x_j, y_j\}_i$ is constructed by filtering $\gt{f}$ using a Gaussian filter (Eq~\ref{e-gaussian-filter}), we should replace $\norm{g_i(x_j)-y_j}$ by $\norm{f_i(x_j)-y_j}$ in Eq~\ref{e-loss}, recall that $f_i$ is the $i$th level of detail of the MR-Net $f$, i.e. $f_i=g_0+\dots+g_{i}$.

% Consequently, a good loss function bridges the gap between the data and the functional model.
% The most common \textit{norms} for regression problems use the $L^1$ and $L^2$ norms.

During training, the network can be over-fitted to the data. To avoid this, we explore regularization strategies such as defining convergence criteria for early stopping to fit the network to the data. In the future, we plan to enhance these strategies by adding \textit{regularization terms} based on network derivatives. 


% Notice that we are working with representational neural networks, that is, a neural network that is trained to represent a single signal. This way, the over-fitting and generalization problem.

\subsubsection{Logger}

The training of the MR-Net $f$ is monitored by a logging module that helps to visualize the learning progress. During training, the logger receives messages when the network training starts and ends, when a stage training starts and ends, when a epoch training ends, and when a batch training ends. 

This architecture allows the logger to be customized to accommodate different MR-Net configurations and different actions for visualizing the training progress. For instance, it is possible to write a logger to save partial results to the disk, display~them in a development environment, or send them to a cloud based~service.


\subsubsection{Scheduler}

Since a MR-Net $f$ has $N$ stages $g_i$, each learning a level of detail of the ground-truth signal $\gt{f}$, one important aspect is their training schedule. 
If the input multi-stage stack $\{x_j, y_j\}_i$ is organized as a Laplacian pyramid and $f$ is a L-Net, it is possible to train all stages in parallel by summing the loss functions $\mathcal{L}_i$ of the MR-stages. However, in general, it may be beneficial to train each stage sequentially, from the lowest to the highest level. This scheduling is our choice and a common strategy in the traditional multiresolution analysis of signals.

Furthermore, we adopt a progressive learning strategy by "freezing” the weights of a stage once it is trained in the scheduled sequence. This strategy guarantees that the details are added to the representation incrementally from coarse to fine.

We also employ an adaptive training scheme for each stage optimization combining both accuracy loss thresholds and convergence rates. The training process is outlined in Algorithm~\ref{alg:mr-training}.

% \begin{algorithm}[hbt!]
% \caption{MR-Net training.}\label{alg:mr-training}
% \KwData{A multi-stage stack $\{x_j, y_j\}_i$ with $N$ levels.}
% \KwResult{A MR-Net $f$ with $N$ stages $g_i$.}
%     Initialize a MR-Net model with a single stage $g_0$\;
%     Notify Logger that network training will start\;
%     \For{$stage\gets0$ \KwTo $N-1$}{
%         \If{$stage\neq~0$}
%         {
%             Create a new stage $g_{stage}$ and add it to the model\;
%             Freeze the parameters of the stage $g_{stage-1}$\;
%         }
%         Notify Logger that stage training will start\;
%         $\texttt{current\_traindata}~\!\!\gets~\!\!\texttt{multires\_stack}[stage]$\;
%         \For{\normalfont $epoch\gets 0$ \KwTo \texttt{current\_limit\_of\_epochs}}{
%             \For{\normalfont \textit{batch} in \texttt{current\_traindata}}{
%                 Train $g_{stage}$ using the loss $\mathcal{L}_{stage}$ (Eq~\ref{e-loss}) \;
%                 Notify Logger that batch training has finished\;
%             }
%             Notify Logger that epoch training has finished\;
%             \If{\normalfont\texttt{convergence\_criteria\_reached()}}
%                 {\textbf{break}\;}
%         }
%         Notify Logger that stage training has finished\;
%     }
%     Notify Logger that network training has finished\;
% \end{algorithm}


\subsubsection{Level of Detail Schemes}
\label{s:lod}

By combining the different aspects discussed in the previous sections we can define various schemes for learning level of detail representations using MR-Nets. The main ones are: capacity based with original signal; filtering with Gaussian/Laplacian tower; and filtering with Gaussian/Laplacian pyramid.

\paragraph{Capacity Based with Original Signal}~\\
In this scheme, we train each stage $g_i$ of the MR-Net $f$ on the same sampling of the ground-truth signal $\gt{f}$ which we consider to have
no frequency higher than a bandlimit $\omega$.  That is, the input multi-stage stack $\{x_j, y_j\}_i$ is composed of $N$ copies of a sample $\{x_j, y_j\}$ of $\gt{f}$. Thus, the training of each stage $g_i$ receives the same input. This scheme is based on the fact that even when $f$ does not have enough capacity to fit $\gt{f}$, it can learn its lowest frequencies and represent a filtered version of $\gt{f}$. Section \ref{sec-module-capacity} describes the frequency control by network capacity, and Figure \ref{f:capacity-filtering} illustrates this phenomenon by showing the reconstruction in 1D example. The MR-Net used in this case has a single stage with width $16$ and a single layer in all blocks.
\begin{figure}[!h]
\centering
\includegraphics[width=\linewidth]{img/ch4/capacity-filtering.png}
% \vspace{-0.4cm}
\caption{Capacity-filtering in the reconstruction of a 1D signal.}
\label{f:capacity-filtering}
\end{figure}

To initialize the $N$ stages ${g_i}$ of $f$, we follow the scheme presented in Section~\ref{s-frequency-initialization}. We initialize the rows of the first matrix of each stage $g_i$ with values in $\Omega_i=[-\omega_i, \omega_i]^2$, where ${\omega_i}$ is a partition of the interval $[0,\omega]$, and $\omega$ is the bandlimit of $\gt{f}$. Consequently, $g_0$ is initialized and trained to learn the lowest frequencies of $\gt{f}$ up to a limit determined by its capacity. We then add stage $g_1$ to the network to learn more details of $\gt{f}$, and continue adding stages until the $N$th stage is reached. 

Note that, in this scheme, the training data and the test data for each level of detail is the original signal. This way, we can also stop adding stages if a desired error tolerance is achieved.


\paragraph{Filtering with Gaussian / Laplacian Tower}~\\
To have control over the frequencies present in the signal $\gt{f}$, we sample a filtered multi-stage stack $\{x_j, y_j\}_i$ and train the stages $g_i$ of MR-Net $f$ to approximate each level $i$ of this stack.

We start by feeding our model with a \textit{Gaussian tower}, that is, a multi-stage stack $\{x_j, y_j\}_i$ where each level $i$ is a version of the level $i+1$ filtered by a low-pass filter. 
Precisely, the Gaussian tower $\{x_j, y_j\}_i$ is defined recursively by convolving the \textit{level of detail} $\gt{f}_i$ of $\gt{f}$ with a Gaussian kernel $K$:
\begin{align*}
    \gt{f}_i(x_j)&=\left(K*\gt{f}_{i+1}\right)(x_j),\\
    \gt{f}_{N-1}&=\gt{f}.
\end{align*}
Thus, we define $y_j=\gt{f}_i(x_j)$.
This way, each level $i$ is reconstructed with the same amount of samples. The MR-Net $f$ must be trained from the less detailed scale to the most detailed~one using the loss $\mathcal{L}_i$ that minimizes the differences $\norm{f_i(x_j)-y_j}^2$, where $f_i=g_0+\dots+g_{i}$.

Similarly, we could represent the multi-stage stack $\{x_j, y_j\}_i$ as a \textit{Laplacian tower} $\{\gt{g}_i\}$ by using:
\begin{align*}
    \gt{g}_i(x_j)&=\left(\gt{f}_{i}-K*\gt{f}_{i}\right)(x_j),\\
    \gt{g}_0(x_j)&=\left(K*\gt{f}_{1}\right)(x_j).
\end{align*}
Thus, we define $y_j=\gt{g}_i(x_j)$ and train the MR-Net $f$ using the loss $\mathcal{L}_i$ that minimizes the differences $\norm{g_i(x_j)-y_j}^2$.


\paragraph{Filtering with Gaussian / Laplacian Pyramid}~\\
The \textit{Gaussian pyramid} is a classical multiscale representation of uniformly sampled signals. Based on the Shannon sampling theorem, the Gaussian tower is a highly redundant multiscale representation. On the other hand, the Gaussian pyramid is ``critically sampled'', i.e., it has the minimum number of samples required to represent each frequency band. 

Precisely, the Gaussian pyramid $\{x_{k,l}, y_{k,l}\}_i$ is defined by recursively downsampling the above Gaussian tower of the signal $\gt{f}$ by a factor 2.
As in Section~\ref{sec:mr_struct}, we are assuming that the sampled points $\{x_{k,l}\}$ forms a grid of size $2^k\times 2^k$ for some integer $k>N$.
% Thus each dimension of a stage grid $\{x_{k,l}\}_i$ has twice the size of the previous one
Similarly, the Laplacian pyramid is defined using the Laplacian tower of $\gt{f}$. 

While the reconstruction of signals using the Gaussian tower is perfect, it is also wasteful if we can generalize correctly the model based only on the samples of a Gaussian pyramid. In terms of efficiency, it's faster to train the model on fewer samples, aligned with classical sampling theory results. 


When training with a multi-stage stack with grids of different resolutions such as a Gaussian pyramid, we can build another multi-stage stack where each level has the same resolution as the original signal and use it as test data. In this case, this second multi-stage stack, a tower, should have each level filtered accordingly to its corresponding level in the pyramid, so that they are separated in similar frequency bands. With this pre-processing, we can train the network on the multiresolution pyramid data, and evaluate it on the original signal resolution, comparing it against the multiresolution tower data to check if it's generalizing as expected.

% In practice, we assume that the data is given as a regular sample (digital image) of size $2^{k}\times 2^k$
% of the ground-truth signal~$\gt{f}$.
% We abuse the notation by denoting this digital image by~$\gt{f}$.
% To train the MR-Net stages~$\{g_i\}$, we use the (discrete) \textit{Gaussian} and \textit{Laplacian} pyramids of $\gt{f}$, both with $N<k$ levels.
% Precisely, the Gaussian pyramid $\{\gt{f}_i\}$ is defined recursively by convolving the \textit{level of detail} $\gt{f}_i$ with a Gaussian kernel $K$ and downsampling the result by a factor 2:
% \begin{align*}
%     \gt{f}_i(k,l)&=\left(K*\gt{f}_{i+1}\right)(2k,2l)\,\, \text{with} \,\, k,l\in \left\{1,\ldots, 2^{k-N+1+i}\right\},\\
%     \gt{f}_{N-1}&=\gt{f}.
% \end{align*}
% $\gt{f}_i(k,l)$ denotes the digital image $\gt{f}_i$ evaluated at the pixel $(k,l)$.
% Similarly, the Laplacian pyramid $\{\gt{g}_i\}$ is defined using
% \begin{align*}
%     \gt{g}_i(k,l)&=\left(\gt{f}_{i}-K*\gt{f}_{i}\right)(2k,2l)\,\, \text{with} \,\, k,l\in \left\{1,\ldots, 2^{k-N+1+i}\right\}\\ \gt{g}_0(k,l)&=\left(K*\gt{f}_{1}\right)(2k,2l).
% \end{align*}

The training of the MR-Net stages $g_i$ using Gaussian/Laplacian is analogous to the tower's case.
Regarding the initialization of $g_i$, observe that $2^{k-N+1+i}$ is the height and width of the $i$th stack $\{x_{k,l}, y_{k,l}\}_i$, thus it cannot contain frequencies higher than $\omega_i=2^{k-N+i}$. 
Thus we propose to initialize the frequencies of $\{g_i\}$ following a dyadic sequence of frequency bands 
$\omega_{N-1}, \omega_{N-2}, \ldots, \omega_{0},$
which is equivalent to $$\omega_{N-1}, \frac{\omega_{N-1}}{2}, \ldots, \frac{\omega_{N-1}}{2^{N-1}} \text{ with } \omega_{N-1}= 2^{k-1}.$$
These are the bandlimits used to define the sets $\Omega_i=\left[-\omega_i, \omega_i\right]^2$ to initialize the frequencies of the first layer of each stage $g_i$.


\subsection{MR-Inference}

Arguably, the primordial purpose of a signal representation is to provide an accurate reconstruction of the underlying data. Moreover, in the ideal case, the reconstruction method should be able to work with a continuous model of the signal, generating signal values at arbitrary points of its domain.

In that respect, coordinate-based neural networks features a compact model of the signal as a continuous function. Additionally, our MR-Net architecture gives a representation that is continuous both at space and scale. 
Therefore, it can reconstruct the signal zooming in and out at any desired level of detail by specifying a value $t$ to adjust the control layer coefficients, during the inference, according to Equations \ref{e-mrnet} and \ref{e-control}.

These characteristics are very important in media applications. In particular, there is a need to control the signal reconstruction for rendering, thus making it adapted to display resolution.


\subsubsection{Antialiasing and Progressive Processing}

The MR-Net architecture subsumes a model that incorporates filtering of the signal's frequency content in a controlled manner. This capability is crucial for antialiasing, necessary to avoid visual artifacts when rendering the signal.

The MR-Net representation as a hierarchy of levels of detail has implications for transmission and processing of the signal. On one hand, regarding the former, it is possible to send coarse versions of the signal, quickly through a channel and subsequently update the level of detail for progressive renderings. On the other hand, concerning the latter, level of detail facilitates data caching using the different memory structures of the GPU.
