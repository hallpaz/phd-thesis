\lhead{\emph{Introduction}}
\chapter{Introduction}

\section{Vision}

Over the past decade, the computer science community has seen an explosion in research in neural networks, motivated mainly by advances in Deep Learning~\citep{lecun2015deep,goodfellow2016deep}. Machine learning has been initially introduced in Computer Vision as a tool for solving analysis tasks such as object detection \citep{redmon2016you}, instance segmentation \citep{he2017mask}, pose estimation \citep{cao2019openpose} and other scene understanding tasks \citep{eigen2014depth, long2015fully, vinyals2015show, guler2018densepose}. 

Given the non-structured and high-dimensional nature of visual data, identifying and engineering relevant features in images, video, and other media has posed significant challenges for traditional methods. Neural networks, particularly deep learning models, have become the go-to solution in computer vision due to their ability to learn hierarchical feature representations directly from raw data. Unlike conventional approaches that rely on hand-crafted features, requiring specialized knowledge and creativity of researchers and engineers, neural networks adaptively optimize feature extraction, often discovering patterns that are too complex for manual engineering. This capability, combined with their scalability across vast datasets, has led to their widespread adoption and popularity in the computer vision community.

As the field progressed, generative models such as Variational Autoencoders (VAEs) \citep{kingma2014auto}, Generative Adversarial Networks (GANs) \citep{goodfellow2014generative}, and more recently, Diffusion Generative Models \citep{ho2020denoising}, have unlocked new possibilities in the creation of media. While initial research concentrated on generating high-quality images \citep{karras2017progressive}, recent work has extended these techniques to other media types, including audio \citep{donahue2018adversarial}, video \citep{vondrick2016generating}, and 3D models \citep{wu2016shapenets}.

In both analysis and generation tasks, deep neural networks have become essential tools for processing large datasets. In analysis, they serve as models that recognize patterns in the data, extract meaningful features, and classify or regress the relevant information. On the generative side, neural networks approximate the underlying probability distribution of the data or implicitly sample from this distribution, generating new examples that retain the core properties of the dataset. We refer to these types of models as \textit{data-based networks}.

More recently, a new trend has emerged in the scientific community: \textit{coordinate-based neural networks}. These networks are not trained to analyze or generate data; instead, they are used to \textit{represent} data or to parameterize transformations, as seen in neural rendering~\citep{starNeuralRendering22}, for example. Coordinate-based networks leverage the universal approximation theorem, which states that neural networks can approximate any continuous function under certain conditions~\citep{HORNIK1989359,cybenko89}. This makes them well-suited to represent data that can be modeled mathematically as functions. For instance, an image can be viewed as a function that maps a two-dimensional domain to a color space, allowing a neural network to approximate this function. Similarly, a 3D surface can be represented as a level set of an implicit function in space. A neural network can learn this mapping from 3D coordinates \((x, y, z)\) to the corresponding field value, which can then be used for visualizing the surface through established methods in computer graphics.

The revolution caused by deep neural networks reached the media industry and motivated the development of new image representations using neural networks. While the data-based network is appropriate for analysis tasks, relying on a discretization of the image, the coordinate-based network is suitable for synthesis, and provides a continuous and compact representation. For its characteristics, there is a growing interest in using these networks in imaging applications~\citep{xie2022neural}. For instance, coordinate-based networks have been successfully applied in image compression~\citep{dupont2021coin} and super-resolution~\citep{czerkawski2021neural}.

As deep learning becomes increasingly prevalent in modern computation, specialized hardware such as Google's Tensor Processing Unit (TPU)~\citep{googleTPU}, Apple's Neural Engine~\citep{appleNeural}, and Tesla's Full Self-Driving Chip~\citep{Talpes2020Tesla} have been developed to accelerate neural network computations across various devices. In this context, we envision a future where media objects—whether audio, images, videos, or 3D models—are encoded directly into neural networks for storage, processing, and new forms of media manipulation. This paradigm shift could revolutionize how media is represented and handled in computing.


\section{Motivation}

\subsection{Media Objects and Neural Media}

We define \textbf{media content} as any form of information intended to be consumed, shared, or experienced by an audience. This includes various types of content, such as visual, auditory, and interactive media. These concepts serve to convey ideas, narratives, knowledge, or emotions and they exist in the real world independently of a computer.

A \textbf{media object}, on the other hand, refers to the digital representation of media content, which can be stored, processed, or transmitted electronically. Media objects take various forms depending on the type of data they represent. For instance:

\begin{itemize}
\item \textbf{Images} are digital representations of visual information, typically stored as two-dimensional arrays of pixel values. They can be raster-based (composed of pixels, as in PNG and BMP) or vector-based (defined by mathematical equations, as in SVG). Raster images may undergo compression using standards like JPEG for lossy compression or PNG for lossless storage.

\item \textbf{Audio} consists of digital representations of sound, stored as sequences of sampled amplitude values in pulse-code modulation (PCM) format (e.g., WAV, FLAC) or as compressed data using perceptual encoding (e.g., MP3, AAC). It can be represented in various forms, including time-domain waveforms and frequency-domain spectrograms.
 
\item \textbf{Videos} is a temporal sequence of images (frames) that, when played back at a certain frame rate, creates the perception of motion. It often includes an accompanying audio track and is stored using formats like MP4 or AVI, with compression codecs such as H.264 or VP9 to reduce file size while maintaining visual fidelity.

\item \textbf{3D models} are digital representations of geometric objects or scenes in three-dimensional space. They are typically defined using a combination of vertices, edges, and faces to form polygonal meshes (e.g., OBJ, STL), or as parametric surfaces and implicit functions. Some formats, such as glTF, also support materials, textures, animations, and skeletal structures to enhance realism. 3D models can be rendered into 2D images, manipulated in real-time applications, or used in simulations and analysis.
\end{itemize}

When representing real-world media content in a computer, we can apply the \textit{Paradigm of the Four Universes} \citep{gomes1995}:

\begin{enumerate}
\item \textbf{Physical Universe}: The real-world objects and phenomena we intend to model.
\item \textbf{Mathematical Universe}: The abstract, continuous description of these objects using mathematical formulations.
\item \textbf{Representation Universe}: The discrete approximations of the objects, where continuous signals are sampled and quantized.
\item \textbf{Implementation Universe}: The realm of concrete data structures and algorithms that map the discrete representations into computable forms.
\end{enumerate}

The transition from the physical universe to the mathematical universe involves creating a \textit{mathematical model} of the media content as a continuous signal. Once modeled, this signal is discretized, bringing it into the representation universe. Finally, in the implementation universe, these discrete representations are stored in data structures suitable for computational purposes. The process of moving from a discrete representation of a signal to its computable form in the implementation universe is called \textbf{encoding}. 


In this work, our goal is to derive a \textit{multiresolution representation of media objects using neural networks}, a concept we will refer to as \textbf{neural media}. 


While mathematical models for a variety of media content are well-established~\citep{ ipcgVelho2014, wenger2013isosurfaces}, traditional media encoding standards today rely on discrete representations. Neural networks, however, approximate the underlying continuous mathematical models of media content, bridging the Mathematical Universe and the Implementation Universe. A key advantage of using neural networks is their ability to reconstruct continuous functions from a finite set of samples, enabling resampling at arbitrary resolutions without the need for computationally expensive interpolation methods.

In this dissertation, we explore how multiresolution theory can be integrated into this framework by developing neural network architectures capable of encoding signals at multiple levels of resolution. A multiresolution representation can address challenges such as zooming and anti-aliasing, while also offering practical benefits for real-world applications. For instance, multiresolution techniques could enhance streaming technologies, allowing media to be transmitted at varying levels of detail, depending on bandwidth limitations. Additionally, they are likely to be effective for media compression, potentially leading to more compact and efficient formats. We also envision that this approach will pave the way for new operations and applications involving neural media, as exemplified by the optimization of seamless material textures in Chapter \ref{chap:seamless-textures}. 

While this work introduces a multiresolution neural framework for media representation, the experimental focus is primarily on images. This choice is motivated by several factors. First, images provide a structured and widely studied domain where multiresolution techniques, such as wavelet transforms \citep{wavelet1984, mallat1989theory} and Gaussian pyramids \citep{burt1983laplacian}, have demonstrated significant advantages. Their spatial continuity and hierarchical nature make them particularly suitable for evaluating progressive detail refinement, which is central to the proposed approach. This perspective aligns with classic theories in vision science, such as Marr's framework for visual processing, where multiscale representations play a fundamental role in extracting meaningful structures from images \citep{marr82}. Moreover, prior research—some of which is cited in this thesis—has successfully applied sinusoidal neural networks \citep{sitzmann2019siren} or multiresolution principles \citep{bacon2021} to these modalities, further supporting the broader applicability of the proposed method.

% Although the framework is designed to be general and can, in principle, be extended to other types of media, such as audio, video, and 3D models, exploring these applications is left for future work.

% A coordinate-based network represents the image \emph{indirectly} using a fully connected \textit{multi-layer perceptron} (MLP) that takes as input a pixel coordinate and outputs a RGB color. These~networks provide a continuous implicit representation for images~\cite{chen2021learning}, and allow for various applications, from Neural Signed Distance Functions~(NeuralSDFs)~\cite{park2019deepsdf} to Neural Radiance Fields~(NeRFs)~\cite{2020nerf}. Since the coordinates are continuous, images can be presented in arbitrary resolution.


\subsection{Research Questions}

In this scenario, the main research questions we aim to address in this dissertation are:

\begin{itemize}
    \item Can sinusoidal neural networks be interpreted through the lens of frequency learning, and how does this relate to traditional multiresolution analysis? 
    \item Is it possible to represent signals, particularly images, at multiple resolutions using a neural network?
    \item Can such multiresolution representations be employed in real-world tasks where tradionally multiscale computations have been employed, such as texture mapping, anti-aliasing, mip-mapping, and media transmission?
\end{itemize}  
  
By investigating these questions, we aim to bridge the theoretical framework of frequency-based learning with well-established multiresolution techniques, instituting a deeper understanding of how neural networks can mimic and enhance these approaches. Moreover, we investigate whether neural networks, specifically sinusoidal architectures, can serve as an effective medium for multiresolution signal representation, a task traditionally handled through techniques like wavelet transforms and Gaussian pyramids. This research also delves into practical applications where traditional multiscale computations are relevant. By leveraging neural networks, we aim to enhance these processes with greater flexibility and efficiency.


\section{Contributions and Papers}

In summary, the primary contributions of this dissertation are:

\begin{itemize}
    \item We conduct a comprehensive study on the initialization of sinusoidal neural networks for representing multiresolution and periodic signals, providing insights into their efficacy.
    \item We design a novel family of neural network architectures specifically tailored for encoding media objects at multiple resolutions using sinusoidal neural networks.
    \item We develop a flexible framework for training multi-stage neural networks, capable of handling various types of multiresolution data.
    \item We demonstrate the application of our architecture in multiresolution image representation, enabling continuous interpolation across both space and scale to effectively address anti-aliasing challenges.
    \item We present a Fourier Series-based initialization strategy for sinusoidal networks and we prove that the network is constrained within the space of periodic functions, despite the composition of sine waves.
    \item We showcase how periodic networks can efficiently encode periodic textures, and present a technique based on the Poisson Equation to generate seamless material textures.
    \item We demonstrate that our architecture can be seamlessly integrated into the  rendering pipeline of textured objects.
\end{itemize}

Moreover, chapters \ref{chap:mr_snn} to \ref{chap:seamless-textures} of this dissertation are based on the following works produced during this doctoral research:

\begin{itemize}
    \item Hallison Paz, Tiago Novello, Vinicius Silva, Guilherme Schardong, Luiz Schirmer, Fabio Chagas, Helio Lopes, and Luiz Velho. Multiresolution neural networks for imaging. In 2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), volume 1, pages 174-179, 2022.
    % \\doi: 10.1109/SIBGRAPI55357.2022.9991765.
    \item Hallison Paz, Daniel Perazzo, Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vinicius da Silva, Daniel Yukimura, Fabio Chagas, Helio Lopes, and Luiz Velho. MR-Net: Multiresolution sinusoidal neural networks. Computers \& Graphics, 2023.
    \item Hallison Paz, Tiago Novello, and Luiz Velho. 2024. Spectral Periodic Networks for Neural Rendering. In ACM SIGGRAPH 2024 Posters (SIGGRAPH '24). Association for Computing Machinery, New York, NY, USA, Article 47, 1-2. https://doi.org/10.1145/3641234.3671087
\end{itemize}


\section{Dissertation Outline}

This dissertation is structured to guide the reader through the theoretical foundations, development, and applications of sinusoidal neural networks in multiresolution media representation. We begin by setting the context of the research, introducing essential concepts and positioning our contributions within the broader field. Following this, we look over the mechanics of sinusoidal neural networks, with particular attention to how their initialization influences the frequencies they can effectively represent. Building on this knowledge, we present our custom architecture, which integrates principles from multiresolution analysis. Finally, we showcase the practical applications of our approach, particularly in multiresolution imaging and material texture representation, which in turn prompted further refinements.

In \textbf{Chapter 2}, we lay the theoretical foundation necessary for understanding the key concepts discussed in this dissertation. This includes an overview of signal processing theory, multiresolution analysis, and an introduction to neural networks, with a specific focus on coordinate-based and sinusoidal neural networks. This chapter provides the terminology and background knowledge required to comprehend the subsequent chapters.

In \textbf{Chapter 3}, we investigate the relationship between the initialization of sinusoidal neural networks and the frequencies they are capable of representing. Following a perspective that we could call "experimental mathematics", we generate hypotheses rooted in classical signal processing theory and verify them through systematic computational experimentation. This investigation leads to important insights about how these networks learn frequency representations, which inform the design of our subsequent architectures.

In \textbf{Chapter 4}, we introduce the Multiresolution [Sinusoidal] Neural Networks (MR-Net), a novel family of architectures designed for encoding media objects across multiple scales. We explain how MR-Net uses the findings from Chapter 3 regarding the frequency dynamics of sinusoidal neural networks and incorporates principles from multiresolution analysis. We also detail the flexible, multistage training approach used to optimize MR-Net for handling multiresolution data effectively.

In \textbf{Chapter 5}, we focus on the application of MR-Net in imaging tasks. We discuss how imaging applications benefit of multiresolution representations, and we show how to use MR-Net for multiresolution encoding of images, for texture magnification and minification and for antialiasing treatment. Additionally, we provide a comparative analysis of MR-Net's performance against existing methods, highlighting its advantages in these applications.

In \textbf{Chapter 6}, we present another initialization scheme for sinusoidal networks, resulting in the development of periodic neural networks. Inspired by the Fourier Series, this initialization constrains the network's learning space to periodic functions, making it particularly effective for encoding periodic patterns such as material textures. We also present a Poisson Equation-based loss function that optimizes non-tileable texture patches into seamless material textures, further showcasing the advantages of this approach.

Finally, in \textbf{Chapter 7}, we explore potential future research directions stemming from MR-Net and related representational networks. We also discuss the limitations of the current work and provide concluding thoughts on its contributions and impact.
