\chapter{Introduction}

\section{Vision}

Over the past decade, the computer science community has seen an explosion in research in neural networks, motivated mainly by advances in Deep Learning~\cite{lecun2015deep,goodfellow2016deep}. Machine learning has been initially introduced in Computer Vision as a tool for solving analysis tasks such object detection \citep{redmon2016you}, instance segmentation \citep{he2017mask}, pose estimation \citep{cao2019openpose} and other scene understanding tasks [CITE]. Given the non-structured nature of the data and the challenges on identifying and engineering features in images, video and other visual representations, neural networks have been employed as the predominant model in vision tasks. This way, deep learning gained a lot of popularity within the computer vision community.

Later on, generative models such as Variational Auto-encoders \citep{kingma2014auto} and Generative Adversarial Networks (GANs) \citep{goodfellow2014generative}, and more recently Diffusion Generative Models \citep{ho2020denoising} have opened new possibilities in the realm of learning to generate a object or a family of objects. While most of the works have concentrated into image generation \citep{karras2017progressive}, there have been exploration in the direction of other media objects such as audio \citep{donahue2018adversarial}, video \citep{vondrick2016generating} or 3D models \citep{wu2016shapenets}.

In both cases, analysis or generation, deep neural network models have been used as a tool to process large datasets. In analysis tasks, they are a model to recognize patterns in the data, extract features from the data and classify or regress some information. In the generative case, they are used to approximate the data probability distribution or implicity sample from this distribution, so that we can have new examples with similar propteties to the dataset. We will call these kinds of networks \textit{data-based networks}. However, more recentely, it has emerged a new trend in the scientific community to use \textit{coordinate-based neural networks}, where the networks are used to \textit{represent} some data or to parameterize transformations (Neural rendering?).

The revolution caused by deep neural networks reached the media industry and motivated the development of new image representations using neural networks. While the data-based network is appropriate for analysis tasks, relying on a discretization of the image, another kind of network called \textit{coordinate-based~network} is suitable for synthesis, and provides a continuous and compact representation. For its characteristics, there is a growing interest in using these networks in imaging applications~\cite{xie2022neural}. For instance, coordinate-based networks have been successfully applied in image compression~\cite{dupont2021coin} and super-resolution~\cite{czerkawski2021neural}.

Deep learning has been so present in computation today that specialized hardware such as a Tensor Processor Unit (Google TPU), Neural Engine (Apple) or Tesla Full Self-Driving Chip has been developed to accelerate computations with neural networks in a variety of devices. In this context, we envision a future where media objects (audio, image, video, 3D models etc) will be encoded in neural networks either for storage, processing or derivation of new operations.

\section{Motivation}

\subsection{Media Objects and Neural Media}

In this context, our goal is to derive a \textit{multiresolution representation of media objects using neural networks}, a concept we will refer to as \textbf{neural media}. We define \textbf{media content} as any form of information intended to be consumed, shared, or experienced by an audience. This includes various types of content, such as visual, auditory, and interactive media. These concepts serve to convey ideas, narratives, knowledge, or emotions and they exist in the real world independently of a computer.

A \textbf{media object}, on the other hand, refers to the digital representation of media content, which can be stored, processed, or transmitted electronically. Media objects take various forms depending on the type of data they represent. For instance:

\begin{itemize}
\item \textbf{Images} are digital representations of still pictures, typically stored as 2D arrays of pixel values (e.g., JPEG, PNG).
\item \textbf{Audio} consists of sound recordings represented as waveforms or spectrograms (e.g., MP3, WAV).
\item \textbf{Videos} are sequences of images combined with an audio track (e.g., MP4, AVI).
\item \textbf{3D models} are geometric representations of objects or scenes (e.g., OBJ, STL).
\end{itemize}

When representing real-world media content in a computer, we can apply the \textit{Paradigm of the Four Universes} (\cite{gomes1995}):

\begin{enumerate}
\item \textbf{Physical Universe}: The real-world objects and phenomena we intend to model.
\item \textbf{Mathematical Universe}: The abstract, continuous description of these objects using mathematical formulations.
\item \textbf{Representation Universe}: The discrete approximations of the objects, where continuous signals are sampled and quantized.
\item \textbf{Implementation Universe}: The realm of concrete data structures and algorithms that map the discrete representations into computable forms.
\end{enumerate}

The transition from the physical universe to the mathematical universe involves creating a \textit{mathematical model} of the media content as a continuous signal. Once modeled, the signal is discretized, bringing it into the representation universe. Finally, in the implementation universe, these discrete representations are stored in data structures suitable for computational purposes.

The process of moving from a discrete representation of a signal to its computable form in the implementation universe is called \textbf{encoding}. In this work, we focus on encoding media objects using \textbf{artificial neural networks}. We will show that neural networks can not only approximate the underlying continuous mathematical model of the media content but also provide an efficient representation of its discrete and finite form. Additionally, we will explore how multiresolution theory can be applied within this framework, developing neural network architectures capable of encoding signals at multiple levels of resolution.


In this context, using neural networks to represent media objects such as audio, image, video or 3D models have attracted attention of the community as it is a compact and continuous representation. 

Nowadays, images can be stored and processed in a variety of file formats. However there are basically two big classes of representations fr

Neural Networks are universal approximators models for functions. All these objects can be described in terms of signals in the Mathematical Universe

A coordinate-based network represents the image \emph{indirectly} using a fully connected \textit{multi-layer perceptron} (MLP) that takes as input a pixel coordinate and outputs a RGB color. These~networks provide a continuous implicit representation for images~\cite{chen2021learning}, and allow for various applications, from Neural Signed Distance Functions~(NeuralSDFs)~\cite{park2019deepsdf} to Neural Radiance Fields~(NeRFs)~\cite{2020nerf}. Since the coordinates are continuous, images can be presented in arbitrary resolution.


% Note that audio, image and videos have a natural representation in euclidean domains, as their digital representations are encoded as lattices, samples with regularity in space and time. However, other media objects such as 3D models have many possible representations available such as point clouds, polygonal meshes, multiview images or implicit surfaces.

% \red{As these two use cases became more "stable", the field kept advancing towards non-euclidean domains. Geometric Deep Learning, Graph Neural Networks .... How to apply machine learning techiniques, more specifically, deep learning, to solve problems in Computer Graphics?}


% Many works have addressed each of these representations. For instance [CITE works for each kind]... At this point, other tasks such as geometry reconstruction and appearance estimation started to be better addressed. Still, most of these works could be either classified as analysis - how to classify an object based on a point cloud - or generative models based: after training on a dataset, given an image as a prior, generate the corresponding geometry.


\subsection{Research Questions}

In this scenario, the main research questions we aim to address in this dissertation are:

\begin{itemize}
    \item Can we interpret sinusoidal neural networks learning in terms of frequencies and relate it to multiresolution analysis? 
    \item Can we represent signals, particularly images, in multiresolution using a neural network?
    \item Would this representation be useful for tasks where tradionally multiscale computations have been employed, for example: texture mapping, anti-aliasing, mip-mapping, storage in the disk and transmission over a network?
\end{itemize}  

\section{Contributions and Papers}

In summary the contributions of this dissertation are:

\begin{itemize}
    \item We provide a study of sinusoidal neural networks initialization for multiresolution and periodic signal representation.
    \item We design a family of architectures of neural networks to encode media objects in multiresolution using sinusoidal neural networks.
    \item We develop flexible framework for training multi-stage neural networks using different types of multiresolution data.
    \item We demonstrate how our architecture can be applied to multiresolution image representation and continuous interpolation in space and scale to solve anti-aliasing.
    \item We further present a initialization strategy for sinusoidal networks based on the Fourier Series to represent periodic signals. We prove that this initialization schemes that the network will be constrained to the space of periodic functions, despite the composition of sines. 
    \item We demonstrate aplication of our periodic networks to encode periodic textures more efficiently and we develop a techinique based on the Poisson Equation to create seamless material textures.
    \item We demonstrate that our architecture can be seamlessly integrated in the  rendering pipeline of textured objects.
\end{itemize}

Moreover, chapters \ref{chap:mr_snn} to \ref{chap:seamless-textures} of this dissertation are mainly based on the follwoing works produced during this doctoral research:

\begin{itemize}
    \item Hallison Paz, Tiago Novello, Vinicius Silva, Guilherme Schardong, Luiz Schirmer, Fabio Chagas, Helio Lopes, and Luiz Velho. Multiresolution neural networks for imaging. In 2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), volume 1, pages 174-179, 2022. 
    % \\doi: 10.1109/SIBGRAPI55357.2022.9991765.
    \item Hallison Paz, Daniel Perazzo, Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vinicius da Silva, Daniel Yukimura, Fabio Chagas, Helio Lopes, and Luiz Velho. Mr-net: Multiresolution sinusoidal neural networks. Computers \& Graphics, 2023.
    \item Hallison Paz, Tiago Novello, and Luiz Velho. 2024. Spectral Periodic Networks for Neural Rendering. In ACM SIGGRAPH 2024 Posters (SIGGRAPH '24). Association for Computing Machinery, New York, NY, USA, Article 47, 1-2. https://doi.org/10.1145/3641234.3671087
\end{itemize}


\section{Dissertation Outline}

This dissertation starts by giving context to the reader, positioning this research in the field, and providing elementary concepts for better understanding of our work. After that, we have a couple of chapter where we discuss how sinusoidal neural networks learn, with particular attention to the relationship betweeen their initialization and the frequencies that they can represent. We use this knowlegde to derive our own architecture, connected with concepts of the multiresolution analysis. Finaly, we present applications of our proposal in multiresolution imaging and material textures representation. These applications motivated new improvements in our approach. 

In Chapter 2, we provide a theoretical framework to help the reader with the terminology we use in this dissertation and give a basic understanding about the foundational topics we are buildng on top. We discuss established knowledge about signal processing, multiresolution analysis and neural networks, particularly sinusoidal neural networks and coordinate-based neural networks.

In Chapter 3, we present an investigation on how sinusoidal neural networks initialization is related to the frequencies and signal representations learned by the model. Following a perspective that we could call \textit{experimental mathematics}, we generate hypothesis and questions grounded in the classical signal processing theory, then we verify how does it apply in sinusoidal neural networks and present the conclusions we can draw after systematic experimentation.

In Chapter 4, we present the Multiresolution [Sinusoidal] Neural Networks (MR-Net), a family of architectures to encode media objects in multiple scales. We discuss how we utilized the findinds about the frequency dynamics in sinusoidal neural networks, and the theory o multiresolution analysis to design MR-Net as a multi-stage architecture, and how we can train such networks in a flexible multiresolution training approach.

In Chapter 5, we present applications of MR-Net in imaging. We discuss how imaging applications benefit of multiresolution representations, and we show how to use MR-Net for multiresolution encoding of images, for texture magnification and minification and for antialiasing treatment. We also compare MR-Net performance in these tasks with previous works in the area.

In Chapter 6, we present a novel initialization scheme for sinusoidal networks, creating the periodic neural networks. Inspired by the Fourier Series, we constrain the space of functions that our architecture is capable of learning to the space of periodic functions and we demonstrante the advantages of this approach for encoding periodic patterns, such as material textures. Also, we design a loss function, based on the Poisson equation, to optmize non-titleable patches into seamless material textures.

Finally, in Chapter 7, we discuss interesting directions of research based on MR-Net or representational networks, and we discuss limitations and a conclusion to this work.

