\chapter{Introduction}

\red{Nice introductory text here identifying the gaps and the research question.}

Machine learning has been initially introduced in Computer Vision as a tool for solving analysis tasks such object detection [CITE], instance segmentation [CITE], pose estimation [CITE] and other scene understanding tasks [CITE]. Given the non-structured nature of the data and the challenges on identifying and engineering features in images, video and other visual representations, neural networks have been employed as the predominant model in vision tasks. This way, deep learning gained a lot of popularity within the computer vision community.

Later on, generative models such as Variational Auto-encoders [CITE] and Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, and more recently Diffusion Generative Models \cite{ho2020denoising} have opened new possibilities in the realm of learning to generate a object or a family of objects. While most of the works have concentrated into image generation [CITE], there have been exploration in the direction of other media objects such as audio [CITE], video [CITE] or 3D models [CITE].


Note that audio, image and videos have a natural representation in euclidean domains, as their digital representations are encoded as lattices, samples with regularity in space and time. However, other media objects such as 3D models have many possible representations available such as point clouds, polygonal meshes, multiview images or implicit surfaces.

\red{As these two use cases became more "stable", the field kept advancing towards non-euclidean domains. Geometric Deep Learning, Graph Neural Networks .... How to apply machine learning techiniques, more specifically, deep learning, to solve problems in Computer Graphics?}


Many works have addressed each of these representations. For instance [CITE works for each kind]... At this point, other tasks such as geometry reconstruction and appearance estimation started to be better addressed. Still, most of these works could be either classified as analysis - how to classify an object based on a point cloud - or generative models based: after training on a dataset, given an image as a prior, generate the corresponding geometry.

In this context, using neural networks to represent media objects such as audio, image, video or 3D models have attracted attention of the community as it is a compact and continuous representation. 


\section{Motivation and Vision}

Nowadays, images can be stored and processed in a variety of file formats. However there are basically two big classes of representations fr

However, deep learning has been so present in computation today that specialized hardware such as a Tensor Processor Unit (TPU) has been developed to accelerate computations with neural networks. In this context, we envision a future where media objects (audio, image, video, 3D models etc) will be encoded in neural networks either for starage, processing or derivation of new operations. 

Neural Networks are universal approximators models for functions. All these objects can be described in terms of signals in the Mathematical Universe


\section{Research Questions}

- Can we represent signals, particularly images, in multiresolution using a neural network? Would this representation be useful for tasks  where tradionally multiscale computations have been employed, for example: anti-aliasing, mip-mapping, storage

\section{Contributions}

In summary the contributions of this dissertation are:

\begin{itemize}
    \item TODO: A characterization of representational neural networks as an unified view to implicit models and implicit neural fields, distinguishing them from analysis networks and generative networks
    \item An architecture of neural networks to encode media objects in multiresolution using an implicit model representation.
    \item A study of sinusoidal neural networks initialization for multiresolution and periodic signal representation.
    \item A flexible framework for training multi-stage neural networks using different types of multiresolution data.
    \item We demonstrate how our architecture can be applied to multiresolution image representation and continuous interpolation in space and scale to solve anti-aliasing.
    \item We demonstrate how to periodic sinusoidal neural networks to represent periodic textures and we develop a techinique based on Poisson Equation to create seamless material textures.
    \item We demonstrate how to use our architecture for rendering of textured objects and panoramic scenes. [TODO: We also show how to use Neural Rendering to learn an equirectangular representation from multiple sampled images of a scene.]
\end{itemize}

% - Present the experiments in fitting multiresolution signals
%     - 1D Gaussian Tower
%     - 1D Gaussian Pyramid
%     - show that networks is well behaved between samples

% \subsection{Gaussian Pyramid Training}


% After exploring the impact of different values of initialization frequencies, we investigated if we could fit multiple scales of the same signal by building a Gaussian pyramid of it. We filtered the signal using a box filter of dimension 5, decimated it by a factor of 2 and used this subsampled version to train our network. Then, we used the trained network to predict the values over all originally sampled points, so we could verify its behaviour on unsupervised points. As we are filtering the higher frequencies of the signal, we expect to be able to represent it exactly by using fewer samples, acconding to the classical sampling theory.  
% We compared our result to a smoothed but not decimated version of the signal where we doubled the size of the filter as we walked to coarse scales. The panel of figures below shows the result of fitting a signal in 7 different scales, starting with \omega_0=256 and dividing it by 2 as we walk to coarse scales.
% We trained the network using less points at each scale.

% \begin{figure}[!htb]
% \minipage{0.49\textwidth}
% \includegraphics[width=\linewidth]{charts/Section-20-Panel-0-ulotlhctz}
% \caption{}
% \endminipage\hfill
% \minipage{0.49\textwidth}
% \includegraphics[width=\linewidth]{charts/Section-20-Panel-1-twwld6gku}
% \caption{}
% \endminipage
% \end{figure}


% \chapter{Multiresolution Sinusoidal Neural Networks}
% \label{ch:mrnet}

% In recent years, the computer science community has seen an explosion in research in neural networks, motivated mainly by advances in Deep Learning~\cite{lecun2015deep,goodfellow2016deep}.
% For visual computing, this was spurred by the creation of Convolutional Neural Networks~(CNNs)~\cite{cnn98}, which had a significant impact both for the research community and the society at large~\cite{li2021survey,shamsaldin2019study}.
% The effectiveness of CNNs comes from the translation invariant properties of the convolution operator, which makes it a proper architecture for the analysis of visual imagery.

% Deep neural networks such as CNNs, employ an array-based discrete representation of the underlying signal. In this case, the network input consists of a vector of pixel values (in RGB) representing the image \emph{directly} by data samples. We call this kind of network a \textit{data-based} network.


% Moreover, the revolution in the media industry caused by deep neural networks motivated the development of new image representations using neural networks. While the data-based network is appropriate for analysis tasks, relying on a discretization of the image, another kind of network called \textit{coordinate-based~network} is suitable for synthesis, and provides a continuous and compact representation. For its characteristics, there is a growing interest in using these networks in imaging applications~\cite{xie2022neural}.
% For instance, coordinate-based networks have been successfully applied in image compression~\cite{dupont2021coin} and super-resolution~\cite{czerkawski2021neural}.


% A coordinate-based network represents the image \emph{indirectly} using a fully connected \textit{multi-layer perceptron} (MLP) that takes as input a pixel coordinate and outputs a RGB color. These~networks provide a continuous implicit representation for images~\cite{chen2021learning}, and allow for various applications, from Neural Signed Distance Functions~(NeuralSDFs)~\cite{park2019deepsdf} to Neural Radiance Fields~(NeRFs)~\cite{mildenhall2021nerf}. Since the coordinates are continuous, images can be presented in arbitrary resolution.

% % Sinusoidal neural networks are particularly suited to model stationary or quasi-stationary signals due to the periodic nature of its activation function~\cite{chen2022}.

% Sinusoidal neural networks are examples of coordinate-based networks in which their activation function is the sine function. As such, they bridge the gap between the spatial and spectral domains, given the close relationship of the sine function with the Fourier~basis. However, these sinusoidal neural networks have been regarded as difficult to train~\cite{taming2017}. To overcome this problem, \citet{sitzmann2019siren} proposed a sinusoidal network for signal representation called SIREN. One of the key contributions of this work is the initialization scheme that guarantees stability and good convergence. Furthermore, it also allows modeling fine details in accordance with the signal’s frequency content.

% A \textit{multiplicative filter network} (MFN) is a sinusoidal network simpler than SIREN which is equivalent to a shallow sinusoidal network~\cite{fathony2020multiplicative}. \citet{bacon2021} presented \textit{band-limited coordinate network }(BACON), an MFN that produces intermediate outputs with an analytical spectral bandwidth (specified at initialization) and achieves multiresolution of the underlying signal. While its structure allows BACON to be expressed as a linear combinations of sines, avoiding the composition of sines present in sinusoidal MLPs, it creates multiresolution representations by truncating the frequency spectra of the signals. This approach produces ringing artifacts  in some levels of detail, and becomes evident when we look at the Fourier transform of the images.

% The control of frequency bands in the representation is closely related with the capability of adaptive reconstruction of the signal in multiple levels of detail.
% In that context, \citet{mueller2022instant} developed a multiresolution neural network architecture based on hash encoding. Also, \citet{martel2021acorn} designed an adaptive coordinate network for neural signals.

% In this context, we introduce \textit{multiresolution sinusoidal neural networks}~(MR-Net) based on classical signal multiresolution representations. 
% Our results, presented in Section~\ref{sub:spectra-eval}, indicate that using MR-Net produces better results compared to the previous state-of-the-art technique, BACON, while employing a smaller number of parameters.
% We describe three MR-Net subclasses: S-Net, L-Net and M-Net.
% Finally, we present applications on antialiasing and level-of-detail reconstruction.

% - Related Works: Mip-Nerf; Bacon; 

% Recently, neural networks have been employed to represent signals directly as functions of spatial coordinates, bypassing traditional grid-based representations. These coordinate-based neural networks (Tancik et al., 2020; Sitzmann et al., 2020) are capable of representing continuous signals in a compact, implicit form, which contrasts with the discrete representations used in classical techniques. Neural Implicit Representations (NIRs), such as Neural Radiance Fields (NeRF) (Mildenhall et al., 2020) and Implicit Neural Representations (INRs), encode spatial information using neural networks where the input coordinates map directly to the desired signal value, such as pixel intensity or 3D occupancy.

% One limitation of these approaches, however, is their tendency to struggle with high-frequency signal components, which has led to research into more robust techniques for handling fine-grained details. In this context, sinusoidal activation functions, as introduced by SIREN (Sitzmann et al., 2020), have proven effective for encoding high-frequency details, motivating further exploration into neural networks that incorporate periodic activations to better capture multiscale information.

Certainly! Here’s a draft of the “Related Works” section for your chapter on *Multiresolution Imaging*. This section will connect your research to prior work in the field, highlighting the evolution of multiresolution techniques in both classical and modern contexts, while positioning your contribution in relation to these.

---

% ### Related Works  
% \label{sec:related}

% The field of multiresolution imaging has a rich history, with methods that span classical signal processing techniques and more recent neural network-based approaches. Understanding the evolution of these methods provides a foundation for contextualizing the contributions of MR-Net, the architecture introduced in this dissertation.

% One of the earliest and most influential contributions to multiresolution imaging is the image pyramid, introduced by \citet{burt1987laplacian}. The Laplacian pyramid decomposes an image into progressively smaller sub-images, capturing different levels of detail through successive downsampling and filtering. This method laid the groundwork for many applications in image compression, texture synthesis, and antialiasing, offering a highly efficient way to represent an image across multiple scales.

% Building upon these early efforts, the wavelet transform emerged as a key multiresolution technique, providing a more versatile framework for signal analysis. \citet{mallat1989theory} introduced the multiresolution analysis (MRA) of signals, which offered a powerful mathematical tool for representing functions at various levels of resolution. Wavelets allow for localized analysis in both the time (or spatial) and frequency domains, making them well-suited for image processing tasks. This method was particularly impactful in the development of image codecs, with the wavelet-based JPEG 2000 standard being a notable example \citep{marcellin2000overview}.

% While wavelets and pyramids dominated early multiresolution techniques, Fourier-based methods have continued to play an important role, especially in the context of frequency decomposition. The Discrete Cosine Transform (DCT) \citep{dct-og} has been widely used in image compression algorithms, such as JPEG, to represent the frequency content of an image and discard less perceptually important coefficients. The ability to perform precise frequency analysis makes sinusoidal functions, as used in the DCT and Fourier transforms, central to understanding and controlling signal fidelity in multiresolution frameworks.

% In recent years, the advent of deep learning has revolutionized multiresolution analysis in the domain of image processing. Coordinate-based neural networks, such as those employing implicit neural representations \citep{chen2019learning, sitzmann2020implicit}, have demonstrated the ability to encode complex signals without relying on traditional grid-based representations. These neural networks can represent functions in a continuous manner, offering flexibility in resolution and scalability. Notably, the use of sinusoidal activations in neural networks has shown promise for controlling the frequency spectrum of learned representations, as demonstrated by works like \citet{sitzmann2020siren} and \citet{tancik2020fourier}, where coordinate-based networks were employed for high-quality signal reconstruction.

% The architecture presented in this dissertation, MR-Net, extends these ideas by integrating sinusoidal activation functions with a multiresolution hierarchical structure. MR-Net allows for the progressive refinement of signal details across scales, echoing the core ideas of wavelets while leveraging the flexibility and power of neural networks. Prior works have explored the use of sinusoidal networks for specific tasks, such as image representation and reconstruction \citep{sitzmann2020siren}, but MR-Net distinguishes itself by providing a framework that explicitly controls frequency generation during training, thus enhancing multiresolution capabilities. This framework is also versatile, with applications ranging from image compression and texture synthesis to neural rendering.

% By building on classical multiresolution techniques and introducing modern neural architectures, MR-Net bridges the gap between traditional signal processing and contemporary machine learning-based approaches. The following sections will discuss the experimental setup and results, demonstrating how MR-Net offers superior performance and flexibility in a variety of imaging tasks.

---

This draft positions your work within the broader context of multiresolution imaging, showing how MR-Net is informed by past methods while also offering a novel approach. Let me know how this fits with your chapter, or if you’d like further adjustments!