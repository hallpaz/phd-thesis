\chapter{Introduction}

\red{Nice introductory text here identifying the gaps and the research question.}

Machine learning has been initially introduced in Computer Vision as a tool for solving analysis tasks such object detection [CITE], instance segmentation [CITE], pose estimation [CITE] and other scene understanding tasks [CITE]. Given the non-structured nature of the data and the challenges on identifying and engineering features in images, video and other visual representations, neural networks have been employed as the predominant model in vision tasks. This way, deep learning gained a lot of popularity within the computer vision community.

Later on, generative models such as Variational Auto-encoders [CITE] and Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, and more recently Diffusion Generative Models \cite{ho2020denoising} have opened new possibilities in the realm of learning to generate a object or a family of objects. While most of the works have concentrated into image generation [CITE], there have been exploration in the direction of other media objects such as audio [CITE], video [CITE] or 3D models [CITE].


Note that audio, image and videos have a natural representation in euclidean domains, as their digital representations are encoded as lattices, samples with regularity in space and time. However, other media objects such as 3D models have many possible representations available such as point clouds, polygonal meshes, multiview images or implicit surfaces.

\red{As these two use cases became more "stable", the field kept advancing towards non-euclidean domains. Geometric Deep Learning, Graph Neural Networks .... How to apply machine learning techiniques, more specifically, deep learning, to solve problems in Computer Graphics?}


Many works have addressed each of these representations. For instance [CITE works for each kind]... At this point, other tasks such as geometry reconstruction and appearance estimation started to be better addressed. Still, most of these works could be either classified as analysis - how to classify an object based on a point cloud - or generative models based: after training on a dataset, given an image as a prior, generate the corresponding geometry.

In this context, using neural networks to represent media objects such as audio, image, video or 3D models have attracted attention of the community as it is a compact and continuous representation. 


\section{Motivation and Vision}

Nowadays, images can be stored and processed in a variety of file formats. However there are basically two big classes of representations fr

However, deep learning has been so present in computation today that specialized hardware such as a Tensor Processor Unit (TPU) has been developed to accelerate computations with neural networks. In this context, we envision a future where media objects (audio, image, video, 3D models etc) will be encoded in neural networks either for starage, processing or derivation of new operations. 

Neural Networks are universal approximators models for functions. All these objects can be described in terms of signals in the Mathematical Universe


\section{Research Questions}

- Can we represent signals, particularly images, in multiresolution using a neural network? Would this representation be useful for tasks  where tradionally multiscale computations have been employed, for example: anti-aliasing, mip-mapping, storage

\section{Contributions}

In summary the contributions of this dissertation are:

\begin{itemize}
    \item TODO: A characterization of representational neural networks as an unified view to implicit models and implicit neural fields, distinguishing them from analysis networks and generative networks
    \item An architecture of neural networks to encode media objects in multiresolution using an implicit model representation.
    \item A study of sinusoidal neural networks initialization for multiresolution and periodic signal representation.
    \item A flexible framework for training multi-stage neural networks using different types of multiresolution data.
    \item We demonstrate how our architecture can be applied to multiresolution image representation and continuous interpolation in space and scale to solve anti-aliasing.
    \item We demonstrate how to periodic sinusoidal neural networks to represent periodic textures and we develop a techinique based on Poisson Equation to create seamless material textures.
    \item We demonstrate how to use our architecture for rendering of textured objects and panoramic scenes. [TODO: We also show how to use Neural Rendering to learn an equirectangular representation from multiple sampled images of a scene.]
\end{itemize}

% - Present the experiments in fitting multiresolution signals
%     - 1D Gaussian Tower
%     - 1D Gaussian Pyramid
%     - show that networks is well behaved between samples

% \subsection{Gaussian Pyramid Training}


% After exploring the impact of different values of initialization frequencies, we investigated if we could fit multiple scales of the same signal by building a Gaussian pyramid of it. We filtered the signal using a box filter of dimension 5, decimated it by a factor of 2 and used this subsampled version to train our network. Then, we used the trained network to predict the values over all originally sampled points, so we could verify its behaviour on unsupervised points. As we are filtering the higher frequencies of the signal, we expect to be able to represent it exactly by using fewer samples, acconding to the classical sampling theory.  
% We compared our result to a smoothed but not decimated version of the signal where we doubled the size of the filter as we walked to coarse scales. The panel of figures below shows the result of fitting a signal in 7 different scales, starting with \omega_0=256 and dividing it by 2 as we walk to coarse scales.
% We trained the network using less points at each scale.

% \begin{figure}[!htb]
% \minipage{0.49\textwidth}
% \includegraphics[width=\linewidth]{charts/Section-20-Panel-0-ulotlhctz}
% \caption{}
% \endminipage\hfill
% \minipage{0.49\textwidth}
% \includegraphics[width=\linewidth]{charts/Section-20-Panel-1-twwld6gku}
% \caption{}
% \endminipage
% \end{figure}


% \chapter{Multiresolution Sinusoidal Neural Networks}
% \label{ch:mrnet}

% In recent years, the computer science community has seen an explosion in research in neural networks, motivated mainly by advances in Deep Learning~\cite{lecun2015deep,goodfellow2016deep}.
% For visual computing, this was spurred by the creation of Convolutional Neural Networks~(CNNs)~\cite{cnn98}, which had a significant impact both for the research community and the society at large~\cite{li2021survey,shamsaldin2019study}.
% The effectiveness of CNNs comes from the translation invariant properties of the convolution operator, which makes it a proper architecture for the analysis of visual imagery.

% Deep neural networks such as CNNs, employ an array-based discrete representation of the underlying signal. In this case, the network input consists of a vector of pixel values (in RGB) representing the image \emph{directly} by data samples. We call this kind of network a \textit{data-based} network.


% Moreover, the revolution in the media industry caused by deep neural networks motivated the development of new image representations using neural networks. While the data-based network is appropriate for analysis tasks, relying on a discretization of the image, another kind of network called \textit{coordinate-based~network} is suitable for synthesis, and provides a continuous and compact representation. For its characteristics, there is a growing interest in using these networks in imaging applications~\cite{xie2022neural}.
% For instance, coordinate-based networks have been successfully applied in image compression~\cite{dupont2021coin} and super-resolution~\cite{czerkawski2021neural}.


% A coordinate-based network represents the image \emph{indirectly} using a fully connected \textit{multi-layer perceptron} (MLP) that takes as input a pixel coordinate and outputs a RGB color. These~networks provide a continuous implicit representation for images~\cite{chen2021learning}, and allow for various applications, from Neural Signed Distance Functions~(NeuralSDFs)~\cite{park2019deepsdf} to Neural Radiance Fields~(NeRFs)~\cite{mildenhall2021nerf}. Since the coordinates are continuous, images can be presented in arbitrary resolution.

% % Sinusoidal neural networks are particularly suited to model stationary or quasi-stationary signals due to the periodic nature of its activation function~\cite{chen2022}.

% Sinusoidal neural networks are examples of coordinate-based networks in which their activation function is the sine function. As such, they bridge the gap between the spatial and spectral domains, given the close relationship of the sine function with the Fourier~basis. However, these sinusoidal neural networks have been regarded as difficult to train~\cite{taming2017}. To overcome this problem, \citet{sitzmann2019siren} proposed a sinusoidal network for signal representation called SIREN. One of the key contributions of this work is the initialization scheme that guarantees stability and good convergence. Furthermore, it also allows modeling fine details in accordance with the signalâ€™s frequency content.

% A \textit{multiplicative filter network} (MFN) is a sinusoidal network simpler than SIREN which is equivalent to a shallow sinusoidal network~\cite{fathony2020multiplicative}. \citet{bacon2021} presented \textit{band-limited coordinate network }(BACON), an MFN that produces intermediate outputs with an analytical spectral bandwidth (specified at initialization) and achieves multiresolution of the underlying signal. While its structure allows BACON to be expressed as a linear combinations of sines, avoiding the composition of sines present in sinusoidal MLPs, it creates multiresolution representations by truncating the frequency spectra of the signals. This approach produces ringing artifacts  in some levels of detail, and becomes evident when we look at the Fourier transform of the images.

% The control of frequency bands in the representation is closely related with the capability of adaptive reconstruction of the signal in multiple levels of detail.
% In that context, \citet{mueller2022instant} developed a multiresolution neural network architecture based on hash encoding. Also, \citet{martel2021acorn} designed an adaptive coordinate network for neural signals.

% In this context, we introduce \textit{multiresolution sinusoidal neural networks}~(MR-Net) based on classical signal multiresolution representations. 
% Our results, presented in Section~\ref{sub:spectra-eval}, indicate that using MR-Net produces better results compared to the previous state-of-the-art technique, BACON, while employing a smaller number of parameters.
% We describe three MR-Net subclasses: S-Net, L-Net and M-Net.
% Finally, we present applications on antialiasing and level-of-detail reconstruction.

% - Related Works: Mip-Nerf; Bacon; 