Traditional digital media representations rely on discrete sampling and often require costly interpolation methods to achieve variable resolutions. In contrast, neural networks offer a continuous framework that bridges mathematical signal representations with computational implementation. Recent advances in deep learning have introduced coordinate-based neural networks, also known as neural fields or implicit neural representations, which harness this framework. Building on these advancements, our research investigates the application of multiresolution theory to coordinate-based neural networks to address challenges in \textit{neural media} representation, including zooming, anti-aliasing, and efficient data transmission.

We propose MR-Net, a family of neural networks for encoding media objects across multiple scales. We design MR-Net through a principled and systematic study of frequency learning in networks with sinusoidal activation functions. Additionally, we introduce a Fourier Series-inspired initialization, proving that it constrains sinusoidal neural networks to periodic function spaces, and show how this representation enables new capabilities, such as seamless material textures, through a Poisson equation-based regularization term added to the loss function. We demonstrate MR-Net's potential in image encoding, anti-aliasing, texture mapping, and seamless material synthesis. Moreover, MR-Net achieves comparable or superior Peak Signal-to-Noise Ratio (PSNR) in image reconstruction tasks relative to previous state-of-the-art models, using fewer parameters. This dissertation thus contributes to bridging multiresolution theory with neural networks, suggesting scalable, practical directions for contemporary media applications, including more complex objects.