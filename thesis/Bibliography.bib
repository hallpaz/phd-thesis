%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "Gerry Murray",
%%%     version         = "1.2",
%%%     date            = "2 April 2012",
%%%     filename        = "acmsmall-sample-bibfile.bib",
%%%     address         = "ACM, NY",
%%%     email           = "murray at hq.acm.org",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "ACM Reference Format, bibliography, citation, references",
%%%     supported       = "yes",
%%%     docstring       = "This BibTeX database file contains 'bibdata' entries
%%%                        that 'match' the examples provided in the Specifications Document
%%%                        AND, also, 'legacy'-type bibs. It should assist authors in
%%%                        choosing the 'correct' at-bibtype and necessary bib-fields
%%%                        so as to obtain the appropriate ACM Reference Format output.
%%%                        It also contains many 'Standard Abbreviations'. "
%%%  }
%%% ====================================================================

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{sitzmann2019siren,
                author = {Sitzmann, Vincent
                          and Martel, Julien N.P.
                          and Bergman, Alexander W.
                          and Lindell, David B.
                          and Wetzstein, Gordon},
                title = {Implicit Neural Representations
                          with Periodic Activation Functions},
                booktitle = {Proc. NeurIPS},
                year={2020}
            }
            
@article{xian2014,
author = {Jun Xian and Song-Hua Li},
title = {Sampling and reconstruction for shift-invariant stochastic processes},
journal = {Stochastics},
volume = {86},
number = {1},
pages = {125-134},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/17442508.2013.763807},
URL = { 
        https://doi.org/10.1080/17442508.2013.763807
    
},
eprint = { 
        https://doi.org/10.1080/17442508.2013.763807
    
}

}

@inproceedings{taming2017,
  title={Taming the waves: sine as activation function in deep neural networks},
  author={G. Parascandolo and H. Huttunen and T. Virtanen},
  year={2017}
}

@inproceedings{bacon2021,
author = {D. Lindell and D. Van Veen and J. Park and G. Wetzstein},
title = {{BACON}: Band-limited coordinate networks for multiscale scene representation},
booktitle = {Proceedings of CVPR},
year={2022}
}

@article{mallat-mr89,
  author={Mallat, S.G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A theory for multiresolution signal decomposition: the wavelet representation}, 
  year={1989},
  volume={11},
  number={7},
  pages={674-693},
  doi={10.1109/34.192463}
}


@article{novello2022differential,
  title={Exploring differential geometry in neural implicits},
  author={Novello, Tiago and Schardong, Guilherme and Schirmer, Luiz and da Silva, Vinicius and Lopes, Helio and Velho, Luiz},
  journal={Computers \& Graphics},
  volume={108},
  pages={49--60},
  year={2022},
  publisher={Elsevier}
}

@article{novello2022neural,
	title = {Neural Implicit Surfaces in Higher Dimension},
	author = {Novello, Tiago and da Silva, Vin\'icius and Schardong, Guilherme and Schirmer,
		Luiz and Lopes, H\'elio and Velho, Luiz},
	journal = {arXiv:2201.09636},
	year = {2022},
	month = jan
}

@article{silva2022mip-plicits,
	title = {Neural Implicit Mapping via Nested Neighborhoods
},
	author = {da Silva, Vin\'icius and Novello, Tiago and Schardong, Guilherme and Schirmer,
		Luiz and Lopes, H\'elio and Velho, Luiz},
	journal = {	arXiv:2201.09147},
	year = {2022}
}

@inproceedings{chen2021learning,
  title={Learning continuous image representation with local implicit image function},
  author={Chen, Yinbo and Liu, Sifei and Wang, Xiaolong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8628--8638},
  year={2021}
}

@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA}
}

@misc{chen2022,
  author = {H. Chen and J. Liu and W. Chen and S. Liu and Y. Zhao},
  title = {Exemplar-based Pattern Synthesis with Implicit Periodic Field Network},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{martel2021acorn,
  title={{ACORN}: {Adaptive} coordinate networks for neural scene representation},
  author={Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
  journal={ACM Trans. Graph. (SIGGRAPH)},
  volume={40},
  number={4},
  year={2021},
}

@article{tancik2020fourfeat,
    title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
    author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
    journal={NeurIPS},
    year={2020}
}

@article{yariv2020multiview,
	  title={Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance},
	  author={Yariv, Lior and Kasten, Yoni and Moran, Dror 
		  and Galun, Meirav and Atzmon, Matan and Ronen, Basri and Lipman, Yaron},
	  journal={Advances in Neural Information Processing Systems},
	  volume={33},
	  year={2020}
	}

@article{Deng2019CvxNetLC,
  title={CvxNet: Learnable Convex Decomposition},
  author={Boyang Deng and Kyle Genova and Soroosh Yazdani and Sofien Bouaziz and Geoffrey E. Hinton and Andrea Tagliasacchi},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={31-41}
}

@article{thies19,
author = {J. Thies and M. Zollh\"{o}fer and M. Nie\ss{}ner},
title = {Deferred Neural Rendering: Image Synthesis Using Neural Textures},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323035},
doi = {10.1145/3306346.3323035},
journal = {ACM Trans. Graph.},
articleno = {66},
numpages = {12},
keywords = {novel view synthesis, facial reenactment, neural rendering, neural texture}
}

@inproceedings{occupancy_networks,
  title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2019},
  doi = {}
}

@inbook{cnn98,
author = {LeCun, Yann and Bengio, Yoshua},
title = {Convolutional Networks for Images, Speech, and Time Series},
year = {1998},
isbn = {0262511029},
publisher = {MIT Press},
booktitle = {The Handbook of Brain Theory and Neural Networks},
pages = {255–258},
numpages = {4}
}

@article{10.1145/74334.74359,
author = {Perlin, K. and Hoffert, E. M.},
title = {Hypertexture},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0097-8930},
url = {https://doi.org/10.1145/74334.74359},
doi = {10.1145/74334.74359},
abstract = {We model phenomena intermediate between shape and texture by using space-filling applicative functions to modulate density. The model is essentially an extension of procedural solid texture synthesis, but evaluated throughout a volumetric region instead of only at surfaces.We have been able to obtain visually realistic representations of such shape+texture (hypertexture) phenomena as hair, fur, fire, glass, fluid flow and erosion effects. We show how this is done, first by describing a set of base level functions to provide basic texture and control capability, then by combining these to synthesize various phenomena.Hypertexture exists within an intermediate region between object and not-object. We introduce a notion of generalized boolean shape operators to combine shapes having such a region.Rendering is accomplished by ray marching from the eye point through the volume to accumulate opacity along each ray. We have implemented our hypertexture rendering algorithms on a traditional serial computer, a distributed network of computers and a coarse-grain MIMD computer. Extensions to the rendering technique incorporating refraction and reflection effects are discussed.},
journal = {SIGGRAPH Comput. Graph.},
month = {jul},
pages = {253–262},
numpages = {10}
}

@inproceedings{hypertexture,
author = {Perlin, K. and Hoffert, E. M.},
title = {Hypertexture},
year = {1989},
isbn = {0897913124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74333.74359},
doi = {10.1145/74333.74359},
abstract = {We model phenomena intermediate between shape and texture by using space-filling applicative functions to modulate density. The model is essentially an extension of procedural solid texture synthesis, but evaluated throughout a volumetric region instead of only at surfaces.We have been able to obtain visually realistic representations of such shape+texture (hypertexture) phenomena as hair, fur, fire, glass, fluid flow and erosion effects. We show how this is done, first by describing a set of base level functions to provide basic texture and control capability, then by combining these to synthesize various phenomena.Hypertexture exists within an intermediate region between object and not-object. We introduce a notion of generalized boolean shape operators to combine shapes having such a region.Rendering is accomplished by ray marching from the eye point through the volume to accumulate opacity along each ray. We have implemented our hypertexture rendering algorithms on a traditional serial computer, a distributed network of computers and a coarse-grain MIMD computer. Extensions to the rendering technique incorporating refraction and reflection effects are discussed.},
booktitle = {Proceedings of the 16th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {253–262},
numpages = {10},
series = {SIGGRAPH '89}
}


@article{mipmap83,
author = {Williams, Lance},
title = {Pyramidal Parametrics},
year = {1983},
issue_date = {July 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0097-8930},
url = {https://doi.org/10.1145/964967.801126},
doi = {10.1145/964967.801126},
journal = {SIGGRAPH Comput. Graph.},
month = {jul},
pages = {1–11},
numpages = {11},
keywords = {Texture mapping, Visible surface algorithms, Modeling, Pyramidal data structures, Antialiasing, Reflectance mapping, Illumination models}
}

@ARTICLE{lena,  author={Munson, David C.},  journal={IEEE Transactions on Image Processing},   title={A note on Lena},   year={1996},  volume={5},  number={1},  pages={3-3},  doi={10.1109/TIP.1996.8100841}}

@techreport{supplemental,
  author = {Luiz Velho and Hallison Paz and Tiago Novello and Daniel Yukimura},
  title = {Multiresolution Neural Networks for Multiscale Signal Representation},
  institution = {VISGRAF Lab},
  year = {2022},
}

@techreport{additional,
  author = {H.Paz and T.Novello and V.Silva and G.Shardong and L.Schirmer and F.Chagas and H.Lopes and L.Velho},
  title = {Multiresolution Neural Networks for Imaging - Additional Material},
  institution = {VISGRAF Lab},
  year = {2022},
}

@article{sierpinski1916,
  title={Sur une courbe cantorienne qui contient une image biunivoque et continue de toute courbe donn{\'e}e},
  author={Sierpinski, Waclaw},
  journal={CR Acad. Sci. Paris},
  volume={162},
  pages={629--632},
  year={1916}
}

@techreport{heckbert1983texture,
  title={Texture mapping polygons in perspective},
  author={Heckbert, Paul S and others},
  year={1983},
  institution={NYIT Computer Graphics Lab}
}


@inproceedings{fathony2020multiplicative,
  title={Multiplicative filter networks},
  author={Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J Zico},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@book{pixel,
  title     = "A Biography of the Pixel",
  author    = "Alvy Ray Smith",
  year      = 2021,
  publisher = "MIT Press",
  address   = "Boston"
}

@inproceedings{Rahaman2018O,
  title={On the Spectral Bias of Neural Networks},
  author={Nasim Rahaman and Aristide Baratin and Devansh Arpit and Felix Dr{\"a}xler and Min Lin and Fred A. Hamprecht and Yoshua Bengio and Aaron C. Courville},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@misc{KodakDataset,
  title = {Kodak Lossless True Color Image Suite},
  howpublished = {\url{http://r0k.us/graphics/kodak/}},
  note = {Accessed: 2023-01-30}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@INPROCEEDINGS{paz2022,  author={Paz, Hallison and Novello, Tiago and Silva, Vinicius and Schardong, Guilherme and Schirmer, Luiz and Chagas, Fabio and Lopes, Helio and Velho, Luiz},  booktitle={2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)},   title={Multiresolution Neural Networks for Imaging},   year={2022},  volume={1},  number={},  pages={174-179},  doi={10.1109/SIBGRAPI55357.2022.9991765}}

@article{novello2022understanding,
  title={Understanding Sinusoidal Neural Networks},
  author={Novello, Tiago},
  journal={arXiv preprint arXiv:2212.01833},
  year={2022}
}

@book{velho2009image,
  title={Image processing for computer graphics and vision},
  author={Velho, Luiz and Frery, Alejandro C and Gomes, Jonas},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@article{lindeberg1994scale,
  title={Scale-space theory: A basic tool for analyzing structures at different scales},
  author={Lindeberg, Tony},
  journal={Journal of applied statistics},
  volume={21},
  number={1-2},
  pages={225--270},
  year={1994},
  publisher={Taylor \& Francis}
}

@book{rosenfeld2013multiresolution,
  title={Multiresolution Image Processing and Analysis},
  author={Rosenfeld, A.},
  isbn={9783642515903},
  lccn={83020074},
  series={Springer Series in Information Sciences},
  url={https://books.google.com.br/books?id=ZGirCAAAQBAJ},
  year={2013},
  publisher={Springer Berlin Heidelberg}
}


@inproceedings{xie2022neural,
  title={Neural fields in visual computing and beyond},
  author={Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
  booktitle={Computer Graphics Forum},
  volume={41},
  number={2},
  pages={641--676},
  year={2022},
  organization={Wiley Online Library}
}


@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{li2021survey,
  title={A survey of convolutional neural networks: analysis, applications, and prospects},
  author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}

@article{shamsaldin2019study,
  title={A study of the convolutional neural networks applications},
  author={Shamsaldin, Ahmed S and Fattah, Polla and Rashid, Tarik A and Al-Salihi, Nawzad K},
  journal={UKH Journal of Science and Engineering},
  volume={3},
  number={2},
  pages={31--40},
  year={2019}
}


@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{park2019deepsdf,
  title={Deepsdf: Learning continuous signed distance functions for shape representation},
  author={Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={165--174},
  year={2019}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@book{mallat1999wavelet,
  title={A wavelet tour of signal processing},
  author={Mallat, St{\'e}phane},
  year={1999},
  publisher={Elsevier}
}



@article{shekarforoush2022residual,
  title={Residual multiplicative filter networks for multiscale reconstruction},
  author={Shekarforoush, Shayan and Lindell, David and Fleet, David J and Brubaker, Marcus A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8550--8563},
  year={2022}
}

@article{yang2022polynomial,
  title={Polynomial neural fields for subband decomposition and manipulation},
  author={Yang, Guandao and Benaim, Sagie and Jampani, Varun and Genova, Kyle and Barron, Jonathan and Funkhouser, Thomas and Hariharan, Bharath and Belongie, Serge},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4401--4415},
  year={2022}
}

@article{stochastic_cook,
author = {Cook, Robert},
year = {1986},
month = {01},
pages = {51-72},
title = {Stochastic Sampling in Computer Graphics},
volume = {5},
journal = {ACM Trans. Graph.},
doi = {10.1145/7529.8927}
}

@article{poisson_bridson,
author = {Bridson, Robert},
year = {2007},
month = {08},
pages = {},
title = {Fast Poisson disk sampling in arbitrary dimensions},
journal = {ACM SIGGRAPH},
doi = {10.1145/1278780.1278807}
}

@book{marr82,
  added-at = {2012-10-25T15:58:41.000+0200},
  address = {New York, NY, USA},
  author = {Marr, David},
  biburl = {https://www.bibsonomy.org/bibtex/2771360e49193a390e85f280dd207de20/daill},
  description = {Vision},
  interhash = {31060780234ce2036de55d261cc63a61},
  intrahash = {771360e49193a390e85f280dd207de20},
  isbn = {0716715678},
  keywords = {computer vision},
  publisher = {Henry Holt and Co., Inc.},
  timestamp = {2012-10-25T15:58:41.000+0200},
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  year = 1982
}



@article{fan2022nerf,
  title={NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes},
  author={Fan, Zhiwen and Wang, Peihao and Jiang, Yifan and Gong, Xinyu and Xu, Dejia and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2209.08776},
  year={2022}
}


@inproceedings{zhu2022nice,
  title={Nice-slam: Neural implicit scalable encoding for slam},
  author={Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R and Pollefeys, Marc},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12786--12796},
  year={2022}
}


@incollection{burt1987laplacian,
  title={The Laplacian pyramid as a compact image code},
  author={Burt, Peter J and Adelson, Edward H},
  booktitle={Readings in computer vision},
  pages={671--679},
  year={1987},
  publisher={Elsevier}
}

@article{dupont2021coin,
  title={Coin: Compression with implicit neural representations},
  author={Dupont, Emilien and Goli{\'n}ski, Adam and Alizadeh, Milad and Teh, Yee Whye and Doucet, Arnaud},
  journal={arXiv preprint arXiv:2103.03123},
  year={2021}
}


@article{czerkawski2021neural,
  title={Neural knitworks: Patched neural implicit representation networks},
  author={Czerkawski, Mikolaj and Cardona, Javier and Atkinson, Robert and Michie, Craig and Andonovic, Ivan and Clemente, Carmine and Tachtatzis, Christos},
  journal={arXiv preprint arXiv:2109.14406},
  year={2021}
}

@book{bracewell1986fourier,
  title={The Fourier transform and its applications},
  author={Bracewell, Ronald Newbold and Bracewell, Ronald N},
  volume={31999},
  year={1986},
  publisher={McGraw-Hill New York}
}

@article{mallat1989theory,
  title={A theory for multiresolution signal decomposition: the wavelet representation},
  author={Mallat, Stephane G},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={11},
  number={7},
  pages={674--693},
  year={1989},
  publisher={Ieee}
}


@inproceedings{marcellin2000overview,
  title={An overview of JPEG-2000},
  author={Marcellin, Michael W and Gormish, Michael J and Bilgin, Ali and Boliek, Martin P},
  booktitle={Proceedings DCC 2000. Data Compression Conference},
  pages={523--541},
  year={2000},
  organization={IEEE}
}

@article{bhaskaran1997image,
  title={Image and video compression standards: algorithms and architectures},
  author={Bhaskaran, Vasudev and Konstantinides, Konstantinos},
  year={1997},
  publisher={Springer Science \& Business Media}
}

@article{antonini1992image,
  title={Image coding using wavelet transform},
  author={Antonini, Marc and Barlaud, Michel and Mathieu, Pierre and Daubechies, Ingrid},
  journal={IEEE Transactions on image processing},
  volume={1},
  number={2},
  pages={205--220},
  year={1992}
}

@ARTICLE{dct-og,
  author={Ahmed, N. and Natarajan, T. and Rao, K.R.},
  journal={IEEE Transactions on Computers}, 
  title={Discrete Cosine Transform}, 
  year={1974},
  volume={C-23},
  number={1},
  pages={90-93},
  doi={10.1109/T-C.1974.223784}}

@inproceedings{mallat-2gen,
author = {Froment, Jacques and Mallat, Ste\'{e}phane},
title = {Second Generation Image Coding and Wavelet Transform},
year = {1996},
isbn = {311013215X},
publisher = {Walter de Gruyter \& Co.},
address = {USA},
booktitle = {Proceedings of the First World Congress on World Congress of Nonlinear Analysts '92, Volume II},
pages = {1923–1932},
numpages = {10},
location = {Tampa, Florida, USA},
series = {WCNA '92}}


%%% ====================================================================
LVELHO
%%% ====================================================================



@inproceedings{peachey-1985,
author = {Peachey, Darwyn R.},
title = {Solid Texturing of Complex Surfaces},
year = {1985},
isbn = {0897911660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/325334.325246},
doi = {10.1145/325334.325246},
abstract = {Texturing is an effective method of simulating surface detail at relatively low cost. Traditionally, texture functions have been defined on the two-dimensional surface coordinate systems of individual surface patches. This paper introduces the notion of "solid texturing". Solid texturing uses texture functions defined throughout a region of three-dimensional space. Many nonhomogeneous materials, including wood and stone, may be more realistically rendered using solid texture functions. In addition, solid texturing can easily be applied to complex surface which are difficult to texture using two-dimensional texture functions. The paper gives examples of solid texture functions based on Fourier synthesis, stochastic texture models, projections of two-dimensional textures, and combinations of other solid textures.},
booktitle = {Proceedings of the 12th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {279–286},
numpages = {8},
keywords = {image synthesis, anti-aliasing, shading, texturing},
series = {SIGGRAPH '85}
}

@article{davies-2002,
author = {Benson, David and Davis, Joel},
title = {Octree Textures},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/566654.566652},
doi = {10.1145/566654.566652},
abstract = {Texturing using a set of two dimensional image maps is an established and widespread practice. However, it has many limitations. Parameterizing a model in texture space can be very difficult, particularly with representations such as implicit surfaces, subdivision surfaces, and very dense or detailed polygonal meshes. This paper proposes the use of a new kind of texture based on an octree, which needs no parameterization other than the surface itself, and yet has similar storage requirements to 2D maps. In addition, it offers adaptive detail, regular sampling over the surface, and continuity across surface boundaries. The paper addresses texture creation, painting, storage, processing, and rendering with octree textures.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {785–790},
numpages = {6},
keywords = {volume texture}
}

@article{BAJAJ-2000,
title = {Compression-Based 3D Texture Mapping for Real-Time Rendering},
journal = {Graphical Models},
volume = {62},
number = {6},
pages = {391-410},
year = {2000},
issn = {1524-0703},
doi = {https://doi.org/10.1006/gmod.2000.0532},
url = {https://www.sciencedirect.com/science/article/pii/S1524070300905320},
author = {Chandrajit Bajaj and Insung Ihm and Sanghun Park},
abstract = {While 2D texture mapping is one of the most effective of the rendering techniques that make 3D objects appear visually interesting, it often suffers from visual artifacts produced when 2D image patterns are wrapped onto the surfaces of objects with arbitrary shapes. On the other hand, 3D texture mapping generates highly natural visual effects in which objects appear carved from lumps of materials rather than laminated with thin sheets as in 2D texture mapping. Storing 3D texture images in a table for fast mapping computations, instead of evaluating procedures on the fly, however, has been considered impractical due to the extremely high memory requirement. In this paper, we present a new effective method for 3D texture mapping designed for real-time rendering of polygonal models. Our scheme attempts to resolve the potential texture memory problem by compressing 3D textures using a wavelet-based encoding method. The experimental results on various nontrivial 3D textures and polygonal models show that high compression rates are achieved with few visual artifacts in the rendered images and a small impact on rendering time. The simplicity of our compression-based scheme will make it easy to implement practical 3D texture mapping in software/hardware rendering systems including real-time 3D graphics APIs such as OpenGL and Direct3D.}
}

@article{hart2002,
author = {Carr, Nathan A. and Hart, John C.},
title = {Meshed Atlases for Real-Time Procedural Solid Texturing},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/508357.508360},
doi = {10.1145/508357.508360},
abstract = {We describe an implementation of procedural solid texturing that uses the texture atlas, a one-to-one mapping from an object's surface into its texture space. The method uses the graphics hardware to rasterize the solid texture coordinates as colors directly into the atlas. A texturing procedure is applied per-pixel to the texture map, replacing each solid texture coordinate with its corresponding procedural solid texture result. The procedural solid texture is then mapped back onto the object surface using standard texture mapping. The implementation renders procedural solid textures in real time, and the user can design them interactively.The quality of this technique depends greatly on the layout of the texture atlas. A broad survey of texture atlas schemes is used to develop a set of general purpose mesh atlases and tools for measuring their effectiveness at distributing as many available texture samples as evenly across the surface as possible. The main contribution of this paper is a new multiresolution texture atlas. It distributes all available texture samples in a nearly uniform distribution. This multiresolution texture atlas also supports MIP-mapped minification antialiasing and linear magnification filtering.},
journal = {ACM Trans. Graph.},
month = {apr},
pages = {106–131},
numpages = {26},
keywords = {Mesh partitioning, solid texturing, texture mapping, MIP-map, procedural texturing, texture atlas}
}


@article{perlin-1985,
author = {Perlin, Ken},
title = {An Image Synthesizer},
year = {1985},
issue_date = {Jul. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {0097-8930},
url = {https://doi.org/10.1145/325165.325247},
doi = {10.1145/325165.325247},
abstract = {We introduce the concept of a Pixel Stream Editor. This forms the basis for an interactive synthesizer for designing highly realistic Computer Generated Imagery. The designer works in an interactive Very High Level programming environment which provides a very fast concept/implement/view iteration cycle.Naturalistic visual complexity is built up by composition of non-linear functions, as opposed to the more conventional texture mapping or growth model algorithms. Powerful primitives are included for creating controlled stochastic effects. We introduce the concept of "solid texture" to the field of CGI.We have used this system to create very convincing representations of clouds, fire, water, stars, marble, wood, rock, soap films and crystal. The algorithms created with this paradigm are generally extremely fast, highly realistic, and asynchronously parallelizable at the pixel level.},
journal = {SIGGRAPH Comput. Graph.},
month = {jul},
pages = {287–296},
numpages = {10},
keywords = {interactive, functional composition, solid texture, fire, space function, turbulence, pixel stream editor, waves, stochastic modelling, algorithm development}
}

@inproceedings{praun-2000,
author = {Praun, Emil and Finkelstein, Adam and Hoppe, Hugues},
title = {Lapped Textures},
year = {2000},
isbn = {1581132085},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/344779.344987},
doi = {10.1145/344779.344987},
abstract = {We present for creating texture over an surface mesh using an example 2D texture. The approach is to identify interesting regions (texture patches) in the 2D example, and to repeatedly paste them onto the surface until it is completely covered. We call such a collection of overlapping patches a lapped texture. It is rendered using compositing operations, either into a traditional global texture map during a preprocess, or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts and drastically reduces texture memory requirements.Through a simple interface, the user specifies a tangential vector field over the surface, providing local control over the texture scale, and for anisotropic textures, the orientation. To paste a texture patch onto the surface, a surface patch is grown and parametrized over texture space. Specifically, we optimize the parametrization of each surface patch such that the tangential vector field aligns everywhere with the standard frame of the texture patch. We show that this optimization is solved efficiently as a sparse linear system.},
booktitle = {Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {465–470},
numpages = {6},
keywords = {texture mapping, texture synthesis, parametrizations},
series = {SIGGRAPH '00}
}

@article{takeo-2022,
author = {Larsson, Maria and Ijiri, Takashi and Yoshida, Hironori and Huber, Johannes A. J. and Fredriksson, Magnus and Broman, Olof and Igarashi, Takeo},
title = {Procedural Texturing of Solid Wood with Knots},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530081},
doi = {10.1145/3528223.3530081},
abstract = {We present a procedural framework for modeling the annual ring pattern of solid wood with knots. Although wood texturing is a well-studied topic, there have been few previous attempts at modeling knots inside the wood texture. Our method takes the skeletal structure of a tree log as input and produces a three-dimensional scalar field representing the time of added growth, which defines the volumetric annual ring pattern. First, separate fields are computed around each strand of the skeleton, i.e., the stem and each knot. The strands are then merged into a single field using smooth minimums. We further suggest techniques for controlling the smooth minimum to adjust the balance of smoothness and reproduce the distortion effects observed around dead knots. Our method is implemented as a shader program running on a GPU with computation times of approximately 0.5 s per image and an input data size of 600 KB. We present rendered images of solid wood from pine and spruce as well as plywood and cross-laminated timber (CLT). Our results were evaluated by wood experts, who confirmed the plausibility of the rendered annual ring patterns.Link to code: https://github.com/marialarsson/procedural_knots.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {45},
numpages = {10},
keywords = {smooth minimum, volumetric texturing, procedural texturing, distance field, natural phenomena, wood}
}

@article{marschner-2016,
author = {Liu, Albert Julius and Dong, Zhao and Ha\v{s}an, Milo\v{s} and Marschner, Steve},
title = {Simulating the Structure and Texture of Solid Wood},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2980179.2980255},
doi = {10.1145/2980179.2980255},
abstract = {Wood is an important decorative material prized for its unique appearance. It is commonly rendered using artistically authored 2D color and bump textures, which reproduces color patterns on flat surfaces well. But the dramatic anisotropic specular figure caused by wood fibers, common in curly maple and other species, is harder to achieve. While suitable BRDF models exist, the texture parameter maps for these wood BRDFs are difficult to author---good results have been shown with elaborate measurements for small flat samples, but these models are not much used in practice. Furthermore, mapping 2D image textures onto 3D objects leads to distortion and inconsistencies. Procedural volumetric textures solve these geometric problems, but existing methods produce much lower quality than image textures. This paper aims to bring the best of all these techniques together: we present a comprehensive volumetric simulation of wood appearance, including growth rings, color variation, pores, rays, and growth distortions. The fiber directions required for anisotropic specular figure follow naturally from the distortions. Our results rival the quality of textures based on photographs, but with the consistency and convenience of a volumetric model. Our model is modular, with components that are intuitive to control, fast to compute, and require minimal storage.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {170},
numpages = {11},
keywords = {BRDF, texture synthesis, solid texture, wood}
}

@article{kopf-2007,
author = {Kopf, Johannes and Fu, Chi-Wing and Cohen-Or, Daniel and Deussen, Oliver and Lischinski, Dani and Wong, Tien-Tsin},
title = {Solid Texture Synthesis from 2D Exemplars},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1276377.1276380},
doi = {10.1145/1276377.1276380},
abstract = {We present a novel method for synthesizing solid textures from 2D texture exemplars. First, we extend 2D texture optimization techniques to synthesize 3D texture solids. Next, the non-parametric texture optimization approach is integrated with histogram matching, which forces the global statistics of the synthesized solid to match those of the exemplar. This improves the convergence of the synthesis process and enables using smaller neighborhoods. In addition to producing compelling texture mapped surfaces, our method also effectively models the material in the interior of solid objects. We also demonstrate that our method is well-suited for synthesizing textures with a large number of channels per texel.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {2–es},
numpages = {10},
keywords = {solid texture, texture synthesis}
}


@article{dorsey-2004,
author = {Jagnow, Robert and Dorsey, Julie and Rushmeier, Holly},
title = {Stereological Techniques for Solid Textures},
year = {2004},
issue_date = {August 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1015706.1015724},
doi = {10.1145/1015706.1015724},
abstract = {We describe the use of traditional stereological methods to synthesize 3D solid textures from 2D images of existing materials. We first illustrate our approach for aggregate materials of spherical particles, and then extend the technique to apply to particles of arbitrary shapes. We demonstrate the effectiveness of the approach with side-by-side comparisons of a real material and a synthetic model with its appearance parameters derived from its physical counterpart. Unlike ad hoc methods for texture synthesis, stereology provides a disciplined, systematic basis for predicting material structure with well-defined assumptions.},
journal = {ACM Trans. Graph.},
month = {aug},
pages = {329–335},
numpages = {7},
keywords = {procedural textures, solid textures, texture synthesis, volumetric textures, spatial sampling theory, stereology}
}

@article{etal-2010,
author = {Lagae, A. and Lefebvre, S. and Cook, R. and DeRose, T. and Drettakis, G. and Ebert, D.S. and Lewis, J.P. and Perlin, K. and Zwicker, M.},
title = {A Survey of Procedural Noise Functions},
journal = {Computer Graphics Forum},
volume = {29},
number = {8},
pages = {2579-2600},
keywords = {procedural noise function, noise, stochastic process, procedural, Perlin noise, wavelet noise, anisotropic noise, sparse convolution noise, Gabor noise, spot noise, surface noise, solid noise, anti-aliasing, filtering, stochastic modelling, procedural texture, procedural modelling, solid texture, texture synthesis, spectral analysis, power spectrum estimation, I.3.3 Computer Graphics: Picture/Image Generation—I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism-Colour, shading, shadowing, and texture},
doi = {https://doi.org/10.1111/j.1467-8659.2010.01827.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2010.01827.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2010.01827.x},
abstract = {Abstract Procedural noise functions are widely used in computer graphics, from off-line rendering in movie production to interactive video games. The ability to add complex and intricate details at low memory and authoring cost is one of its main attractions. This survey is motivated by the inherent importance of noise in graphics, the widespread use of noise in industry and the fact that many recent research developments justify the need for an up-to-date survey. Our goal is to provide both a valuable entry point into the field of procedural noise functions, as well as a comprehensive view of the field to the informed reader. In this report, we cover procedural noise functions in all their aspects. We outline recent advances in research on this topic, discussing and comparing recent and well-established methods. We first formally define procedural noise functions based on stochastic processes and then classify and review existing procedural noise functions. We discuss how procedural noise functions are used for modelling and how they are applied to surfaces. We then introduce analysis tools and apply them to evaluate and compare the major approaches to noise generation. We finally identify several directions for future work.},
year = {2010}
}


@inproceedings{wei-2001,
author = {Wei, Li-Yi and Levoy, Marc},
title = {Texture Synthesis over Arbitrary Manifold Surfaces},
year = {2001},
isbn = {158113374X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383259.383298},
doi = {10.1145/383259.383298},
abstract = {Algorithms exist for synthesizing a wide variety of textures over rectangular domains. However, it remains difficult to synthesize general textures over arbitrary manifold surfaces. In this paper, we present a solution to this problem for surfaces defined by dense polygon meshes. Our solution extends Wei and Levoy's texture synthesis method [25] by generalizing their definition of search neighborhoods. For each mesh vertex, we establish a local parameterization surrounding the vertex, use this parameterization to create a small rectangular neighborhood with the vertex at its center, and search a sample texture for similar neighborhoods. Our algorithm requires as input only a sample texture and a target model. Notably, it does not require specification of a global tangent vector field; it computes one as it goes - either randomly or via a relaxation process. Despite this, the synthesized texture contains no discontinuities, exhibits low distortion, and is perceived to be similar to the sample texture. We demonstrate that our solution is robust and is applicable to a wide range of textures.},
booktitle = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {355–360},
numpages = {6},
keywords = {texture synthesis, curves &amp; surfaces, texture mapping},
series = {SIGGRAPH '01}
}

@article{kobbelt-2020,
author = {Schuster, Kersten and Trettner, Philip and Schmitz, Patric and Kobbelt, Leif},
title = {A Three-Level Approach to Texture Mapping and Synthesis on 3D Surfaces},
year = {2020},
issue_date = {Apr 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
abstract = {We present a method for example-based texturing of triangular 3D meshes. Our algorithm maps a small 2D texture sample onto objects of arbitrary size in a seamless fashion, with no visible repetitions and low overall distortion. It requires minimal user interaction and can be applied to complex, multi-layered input materials that are not required to be tileable. Our framework integrates a patch-based approach with per-pixel compositing. To minimize visual artifacts, we run a three-level optimization that starts with a rigid alignment of texture patches (macro scale), then continues with non-rigid adjustments (meso scale) and finally performs pixel-level texture blending (micro scale). We demonstrate that the relevance of the three levels depends on the texture content and type (stochastic, structured, or anisotropic textures).},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {1},
numpages = {19},
keywords = {surface texture synthesis, texture mapping, material blending}
}


@inproceedings{turk-2004,
author = {Bhat, Pravin and Ingram, Stephen and Turk, Greg},
year = {2004},
month = {07},
pages = {43-46},
title = {Geometric Texture Synthesis by Example.},
journal = {Eurographics Symposium on Geometry Processing},
abstract = {Patterns on real-world objects are often due to variations in geometry across the surface. Height fields and other
common parametric methods cannot synthesize many forms of geometric surface texture such as thorns, scales,
and bark. We present an example-based technique for synthesizing a variety of geometric textures on a model’s
surface. The applied textures can be from models specifically created for this purpose, or may be drawn from
user-specified regions of an example model. We extend existing neighborhood-based texture synthesis algorithms
to operate on volumetric models. Similar to image analogies [11], given a training pair of unfiltered and filtered
source models and an unfiltered destination model (all volumetric grids), we synthesize a filtered fourth model that
exhibits the desired geometric texture. The user defines vector fields to specify the direction of texture anisotropy on
the source and destination models. The vector field defines a coordinate frame on the destination object’s surface
that is used to sample the voxel density values in the neighborhood near a given voxel, which then gives a feature
vector that is matched to the neighborhoods in the source model. Destination voxels are visited in an order that is
dictated by the vector field. We show geometric synthesis results on a variety of models using textures such as pits,
grooves, thru-holes and thorns.},

}

@article{Gutierrez-2019,
	year = 2019,
	month = {nov},
	publisher = {Wiley},
	volume = {39},
	number = {1},
	pages = {511--530},
	author = {J. Gutierrez and J. Rabin and B. Galerne and T. Hurtut},
	title = {On Demand Solid Texture Synthesis Using Deep 3D Networks},
abstract = {This paper describes a novel approach for on demand volumetric texture synthesis based on a deep learning framework that allows for the generation of high quality 3D data at interactive rates. Based on a few example images of textures, a generative network is trained to synthesize coherent portions of solid textures of arbitrary sizes that reproduce the visual characteristics of the examples along some directions. To cope with memory limitations and computation complexity that are inherent to both high resolution and 3D processing on the GPU, only 2D textures referred to as “slices” are generated during the training stage. These synthetic textures are compared to exemplar images via a perceptual loss function based on a pre-trained deep network. The proposed network is very light (less than 100k parameters), therefore it only requires sustainable training (i.e. few hours) and is capable of very fast generation (around a second for 2563 voxels) on a single GPU. Integrated with a spatially seeded PRNG the proposed generator network directly returns an RGB value given a set of 3D coordinates. The synthesized volumes have good visual results that are at least equivalent to the state-of-the-art patch based approaches. They are naturally seamlessly tileable and can be fully generated in parallel.},
	journal = {Computer Graphics Forum}
}

@inproceedings{Zhang-2012,
  author       = {Guo{-}Xin Zhang and
                  Yu{-}Kun Lai and
                  Shi{-}Min Hu},
  editor       = {Shi{-}Min Hu and
                  Ralph R. Martin},
  title        = {Efficient Solid Texture Synthesis Using Gradient Solids},
  booktitle    = {Computational Visual Media - First International Conference, {CVM}
                  2012, Beijing, China, November 8-10, 2012. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {7633},
  pages        = {67--74},
  publisher    = {Springer},
  year         = {2012},
  abstract = {  Abstract. In this paper, we propose a novel solid representation called gradi- ent solids to compactly represent solid textures, including a tricubic interpolation scheme of colors and gradients for smooth variation and a region-based approach for representing sharp boundaries. We further propose a novel approach to di- rectly synthesize gradient solid textures from exemplars. Compared with existing methods, our approach avoids the expensive step of synthesizing the complete solid textures at voxel level and produces optimized solid textures using our rep- resentation. This avoids significant amount of unnecessary computation and stor- age with comparable quality to the state of the art.},
}


@inproceedings {pauly-2009,
booktitle = {Eurographics 2009 - State of the Art Reports},
editor = {M. Pauly and G. Greiner},
title = {{State of the Art in Example-based Texture Synthesis}},
author = {Wie, Li-Yi and Lefebvre, Sylvain and Kwatra, Vivek and Turk, Greg},
year = {2009},
publisher = {The Eurographics Association},
}

@inproceedings{wei-2000,
author = {Wei, Li-Yi and Levoy, Marc},
title = {Fast Texture Synthesis Using Tree-Structured Vector Quantization},
year = {2000},
isbn = {1581132085},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/344779.345009},
doi = {10.1145/344779.345009},
abstract = {Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.},
booktitle = {Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {479–488},
numpages = {10},
keywords = {texture synthesis, image processing, compression algorithms},
series = {SIGGRAPH '00}
}

@article{zhou2018,
author = {Zhou, Yang and Zhu, Zhen and Bai, Xiang and Lischinski, Dani and Cohen-Or, Daniel and Huang, Hui},
title = {Non-Stationary Texture Synthesis by Adversarial Expansion},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201285},
doi = {10.1145/3197517.3201285},
abstract = {The real world exhibits an abundance of non-stationary textures. Examples include textures with large scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {49},
numpages = {13},
keywords = {example-based texture synthesis, non-stationary textures, generative adversarial networks}
}
@inproceedings{bergmann-2017,
author = {Bergmann, Urs and Jetchev, Nikolay and Vollgraf, Roland},
title = {Learning Texture Manifolds with the Periodic Spatial GAN},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Spatial GAN (PS-GAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures, periodic or non-periodic, from datasets of one or more complex large images. Second, we show that the image generation with PS-GANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources, and the method is highly scalable and can generate output images of arbitrary large size.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {469–477},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{berger2017,
title={Incorporating long-range consistency in {CNN}-based texture generation},
author={Guillaume Berger and Roland Memisevic},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HyGTuv9eg}
}

@inproceedings{gatys-2015,
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
title = {Texture Synthesis Using Convolutional Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {262–270},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{rezende-2016,
author = {Rezende, Danilo Jimenez and Eslami, S. M. Ali and Mohamed, Shakir and Battaglia, Peter and Jaderberg, Max and Heess, Nicolas},
title = {Unsupervised Learning of 3D Structure from Images},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A key goal of computer vision is to recover the underlying 3D structure that gives rise to 2D observations of the world. If endowed with 3D understanding, agents can abstract away from the complexity of the rendering process to form stable, disentangled representations of scene elements. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained jointly, end-to-end, and directly from 2D images without any use of ground-truth 3D labels. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {5003–5011},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@INPROCEEDINGS{li-2017,
  author={Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Diversified Texture Synthesis with Feed-Forward Networks}, 
  year={2017},
  volume={},
  number={},
  pages={266-274},
  abstract={Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization.},
  keywords={},
  doi={10.1109/CVPR.2017.36},
  ISSN={1063-6919},
  month={July},}

@INPROCEEDINGS{Gang-2016,
  author={Gang Liu and Gousseau, Yann and Xia, Gui-Song},
  booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
  title={Texture synthesis through convolutional neural networks and spectrum constraints}, 
  year={2016},
  volume={},
  number={},
  pages={3234-3239},
  abstract={This paper presents a significant improvement for the synthesis of texture images using convolutional neural networks (CNNs), making use of constraints on the Fourier spectrum of the results. More precisely, the texture synthesis is regarded as a constrained optimization problem, with constraints conditioning both the Fourier spectrum and statistical features learned by CNNs. In contrast with existing methods, the presented method inherits from previous CNN approaches the ability to depict local structures and fine scale details, and at the same time yields coherent large scale structures, even in the case of quasi-periodic images. This is done at no extra computational cost. Synthesis experiments on various images show a clear improvement compared to a recent state-of-the art method relying on CNN constraints only.},
  keywords={},
  doi={10.1109/ICPR.2016.7900133},
  ISSN={},
  month={Dec},}


@INPROCEEDINGS {tesfaldet-2018,
author = {M. Tesfaldet and M. A. Brubaker and K. G. Derpanis},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Two-Stream Convolutional Networks for Dynamic Texture Synthesis},
year = {2018},
volume = {},
issn = {},
pages = {6703-6712},
abstract = {We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.},
keywords = {dynamics;computational modeling;visualization;spatiotemporal phenomena;streaming media;optical imaging;object recognition},
doi = {10.1109/CVPR.2018.00701},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00701},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{ulyanov-2016,
author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
title = {Texture Networks: Feed-Forward Synthesis of Textures and Stylized Images},
year = {2016},
publisher = {JMLR.org},
abstract = {Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods require a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys et al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1349–1357},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@INPROCEEDINGS {ulyanov-2017,
author = {D. Ulyanov and A. Vedaldi and V. Lempitsky},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {4105-4113},
abstract = {The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.},
keywords = {generators;neural networks;optimization;gallium nitride;convolutional codes;monte carlo methods;entropy},
doi = {10.1109/CVPR.2017.437},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.437},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@article{wilmot-2017,
author = {Wilmot, Pierre and Risser, Eric and Barnes, Connelly},
year = {2017},
month = {01},
pages = {},
title = {Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses}
}

@INPROCEEDINGS {yu-2019,
author = {N. Yu and C. Barnes and E. Shechtman and S. Amirghodsi and M. Lukac},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture},
year = {2019},
volume = {},
issn = {},
pages = {12156-12165},
abstract = {This paper addresses the problem of interpolating visual textures. We formulate this problem by requiring (1) by-example controllability and (2) realistic and smooth interpolation among an arbitrary number of texture samples. To solve it we propose a neural network trained simultaneously on a reconstruction task and a generation task, which can project texture examples onto a latent space where they can be linearly interpolated and projected back onto the image domain, thus ensuring both intuitive control and realistic results. We show our method outperforms a number of baselines according to a comprehensive suite of metrics as well as a user study. We further show several applications based on our technique, which include texture brush, texture dissolve, and animal hybridization.},
keywords = {},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}


@incollection{perez2023poisson,
  title={Poisson image editing},
  author={P{\'e}rez, Patrick and Gangnet, Michel and Blake, Andrew},
  booktitle={Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
  pages={577--582},
  year={2003}
}

@article{schardong2023neural,
  title={Neural implicit morphing of face images},
  author={Schardong, Guilherme and Novello, Tiago and Perazzo, Daniel and Paz, Hallison and Medvedev, Iurii and Velho, Luiz and Gon{\c{c}}alves, Nuno},
  journal={arXiv preprint arXiv:2308.13888},
  year={2023}
}

@inproceedings{yuce2022structured,
  title={A structured dictionary perspective on implicit neural representations},
  author={Y{\"u}ce, Gizem and Ortiz-Jim{\'e}nez, Guillermo and Besbinar, Beril and Frossard, Pascal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19228--19238},
  year={2022}
}

@article{paz2023mr,
  title={MR-Net: Multiresolution sinusoidal neural networks},
  author={Paz, Hallison and Perazzo, Daniel and Novello, Tiago and Schardong, Guilherme and Schirmer, Luiz and da Silva, Vinicius and Yukimura, Daniel and Chagas, Fabio and Lopes, Helio and Velho, Luiz},
  journal={Computers \& Graphics},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{anokhin2021image,
  title={Image generators with conditionally-independent pixel synthesis},
  author={Anokhin, Ivan and Demochkin, Kirill and Khakhulin, Taras and Sterkin, Gleb and Lempitsky, Victor and Korzhenkov, Denis},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14278--14287},
  year={2021}
}

%%% ====================================================================
%% LVELHO


@inproceedings{ntc2023,
    author = {Vaidyanathan, Karthik and Salvi, Marco and Wronski, Bartlomiej and Akenine-Möller, Tomas and Ebelin, Pontus and Lefohn, Aaron},
    title = "{Random-Access Neural Compression of Material Textures}",
    year = {2023},
    booktitle = {Proceedings of SIGGRAPH},
}

@misc{zhou2022tilegen,
      title={TileGen: Tileable, Controllable Material Generation and Capture}, 
      author={Xilong Zhou and Miloš Hašan and Valentin Deschaintre and Paul Guerrero and Kalyan Sunkavalli and Nima Kalantari},
      year={2022},
      eprint={2206.05649},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
}

@article{randcompress,
author = {Vaidyanathan, Karthik and Salvi, Marco and Wronski, Bartlomiej and Akenine-Moller, Tomas and Ebelin, Pontus and Lefohn, Aaron},
title = {Random-Access Neural Compression of Material Textures},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592407},
doi = {10.1145/3592407},
abstract = {The continuous advancement of photorealism in rendering is accompanied by a growth in texture data and, consequently, increasing storage and memory demands. To address this issue, we propose a novel neural compression technique specifically designed for material textures. We unlock two more levels of detail, i.e., 16\texttimes{} more texels, using low bitrate compression, with image quality that is better than advanced image compression techniques, such as AVIF and JPEG XL.At the same time, our method allows on-demand, real-time decompression with random access similar to block texture compression on GPUs, enabling compression on disk and memory. The key idea behind our approach is compressing multiple material textures and their mipmap chains together, and using a small neural network, that is optimized for each material, to decompress them. Finally, we use a custom training implementation to achieve practical compression speeds, whose performance surpasses that of general frameworks, like PyTorch, by an order of magnitude.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {88},
numpages = {25},
keywords = {texture compression, neural networks}
}


@inproceedings{deeptile,
author = {Toulatzis, Vasilis and Fudos, Ioannis},
title = {Deep Tiling: Texture Tile Synthesis Using a Constant Space Deep Learning Approach},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_33},
doi = {10.1007/978-3-030-90439-5_33},
abstract = {Texturing is a fundamental process in computer graphics. Texture is leveraged to enhance the visualization outcome for a 3D scene. In many cases a texture image cannot cover a large 3D model surface because of its small resolution. Conventional techniques like repeating, mirroring or clamping to edge do not yield visually acceptable results. Deep learning based texture synthesis has proven to be very effective in such cases. All deep texture synthesis methods that attempt to create larger resolution textures are limited in terms of GPU memory resources. In this paper, we propose a novel approach to example-based texture synthesis by using a robust deep learning process for creating tiles of arbitrary resolutions that resemble the structural components of an input texture. In this manner, our method is firstly much less memory limited owing to the fact that a new texture tile of small size is synthesized and merged with the existing texture and secondly can easily produce missing parts of a large texture.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {414–426},
numpages = {13},
keywords = {Deep learning, Texture synthesis}
}

@inproceedings{tilehard, author = {Wei, Li-Yi}, title = {Tile-Based Texture Mapping on Graphics Hardware}, year = {2004}, isbn = {3905673150}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1058129.1058138}, doi = {10.1145/1058129.1058138}, abstract = {Texture mapping has been a fundamental feature for commodity graphics hardware. However, a key challenge for texture mapping is how to store and manage large textures on graphics processors. In this paper, we present a tile-based texture mapping algorithm by which we only have to physically store a small set of texture tiles instead of a large texture. Our algorithm generates an arbitrarily large and non-periodic virtual texture map fr\`{o}m the small set of stored texture tiles. Because we only have to store a small set of tiles, it minimizes the storage requirement to a small constant, regardless of the size of the virtual texture. In addition, the tiles are generated and packed into a single texture map, so that the hardware filtering of this packed texture map corresponds directly to the filtering of the virtual texture. We implement our algorithm as a fragment program, and demonstrate performance on latest graphics processors.}, booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware}, pages = {55–63}, numpages = {9}, keywords = {graphics hardware, texture mapping, texture synthesis}, location = {Grenoble, France}, series = {HWWS '04} }

@article{Moritz2017Texture,
    author = {Moritz, J. and James, Stuart and Haines, Tom S. F. and Ritschel, Tobias and Weyrich, Tim},
    title = {Texture Stationarization: Turning Photos Into Tileable Textures},
    journal = {Computer Graphics Forum (Proc. Eurographics)},
    volume = 36,
    number = 2,
    pages = {177--188},
    year = 2017,
    month = may,
    day = 23,
    publisher = {Eurographics Association},
    authorurl = {http://reality.cs.ucl.ac.uk/projects/texture/moritz17texture.html},
}

@inproceedings {egst.exampletexsynth3,
booktitle = {Eurographics 2009 - State of the Art Reports},
editor = {M. Pauly and G. Greiner},
title = {{State of the Art in Example-based Texture Synthesis}},
author = {Wie, Li-Yi and Lefebvre, Sylvain and Kwatra, Vivek and Turk, Greg},
year = {2009},
publisher = {The Eurographics Association},
DOI = {10.2312/egst.20091063}
}

@article {rethinkngtex,
journal = {Computer Graphics Forum},
title = {{Rethinking Texture Mapping}},
author = {Yuksel, Cem and Lefebvre, Sylvain and Tarini, Marco},
year = {2019},
publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
ISSN = {1467-8659},
DOI = {10.1111/cgf.13656}
}

@article{match,
author = {Shi, Liang and Li, Beichen and Ha\v{s}an, Milo\v{s} and Sunkavalli, Kalyan and Boubekeur, Tamy and Mech, Radomir and Matusik, Wojciech},
title = {Match: Differentiable Material Graphs for Procedural Material Capture},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417781},
doi = {10.1145/3414685.3417781},
abstract = {We present MATch, a method to automatically convert photographs of material samples into production-grade procedural material models. At the core of MATch is a new library DiffMat that provides differentiable building blocks for constructing procedural materials, and automatic translation of large-scale procedural models, with hundreds to thousands of node parameters, into differentiable node graphs. Combining these translated node graphs with a rendering layer yields an end-to-end differentiable pipeline that maps node graph parameters to rendered images. This facilitates the use of gradient-based optimization to estimate the parameters such that the resulting material, when rendered, matches the target image appearance, as quantified by a style transfer loss. In addition, we propose a deep neural feature-based graph selection and parameter initialization method that efficiently scales to a large number of procedural graphs. We evaluate our method on both rendered synthetic materials and real materials captured as flash photographs. We demonstrate that MATch can reconstruct more accurate, general, and complex procedural materials compared to the state-of-the-art. Moreover, by producing a procedural output, we unlock capabilities such as constructing arbitrary-resolution material maps and parametrically editing the material appearance.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {196},
numpages = {15},
keywords = {material acquisition, procedural materials}
}

@inproceedings{pattern,
author = {Lefebvre, Sylvain and Neyret, Fabrice},
title = {Pattern Based Procedural Textures},
year = {2003},
isbn = {1581136455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/641480.641518},
doi = {10.1145/641480.641518},
abstract = {Numerous real-time applications such computer games or flight simulators require non-repetitive high-resolution texturing on large landscapes. We propose an algorithm which procedurally determines the texture value at any surface location by aperiodically combining provided patterns according to user-defined controls such as a probability distribution (possibly non stationary). Our algorithm can be implemented on programmable hardware by taking advantage of the texture indirection ability of recent graphics boards. We use explicit and virtual indirection tables to determine the pattern to apply at each pixel as well as its attributes (displacement, scaling, time...). This provides the programmer with a very high resolution virtual texture with nice properties: Low memory consumption, no periodicity, control of the statistics, numerous control parameters (which can be edited on the fly). Our representation consists of building blocks that we combine in order to illustrate various convenient texture modalities such as aperiodic tiling, sparse convolution, domain transitions and animated textures.},
booktitle = {Proceedings of the 2003 Symposium on Interactive 3D Graphics},
pages = {203–212},
numpages = {10},
keywords = {textures, proceduralism, landscape, graphics hardware},
location = {Monterey, California},
series = {I3D '03}
}

@inproceedings{tileinteractive,
author = {Lagae, Ares and Kaplan, Craig S. and Fu, Chi-Wing and Ostromoukhov, Victor and Deussen, Oliver},
title = {Tile-Based Methods for Interactive Applications},
year = {2008},
isbn = {9781450378451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401132.1401254},
doi = {10.1145/1401132.1401254},
abstract = {Over the last years, several techniques have been demonstrated that rely on tile-based methods. A lot of interactive applications could potentially benefit from these techniques. However, the state-of-the-art is scattered over several publications, and survey works are not available. In this class we give a detailed overview of tile-based methods in computer graphics. The class consist of four parts, which are briefly covered in the following paragraphs.Tile-Based Methods using Wang and Corner Tiles The first part of the class introduces tile-based methods in computer graphics based on Wang tiles and corner tiles. This part serves as a general introduction for the class, but also covers methods and applications based on Wang tiles and corner tiles. We introduce Wang tiles and corner tiles, and present several tiling algorithms. We discuss in detail tile-based texture mapping using graphics hardware, tile-based generation of Poisson disk distributions, and object distribution for procedural texturing. We briefly cover other applications such as sampling, non-photorealistic rendering, and geometric object distribution. The lecturer for the first part is Ares Lagae, who recently finished his PhD about tile-based methods in computer graphics [Lagae, 2007].Periodic Tilings for Computer Graphics Applications The second part of the class introduces the mathematical and algorithmic aspects of decorative tilings such as those used by M. C. Escher. It focuses on the theory of isohedral tilings, tilings that cover the plane systematically with congruent copies of a single shape. The isohedral tilings are flexible enough to support a wide variety of applications in art and design, while admitting a compact and efficient implementation. We show how to store, manipulate and render isohedral tilings, and survey some recent applications. The lecturer for the second part is Craig Kaplan, an expert on the use of computer graphics in ornamental design Kaplan [2002].Tile-Based Methods for Surface Modeling The third part of the class covers tilebased methods for surface modeling. Tiling is a practical and cost-effective method for high-quality surface modeling and rendering. Rather than intensive data acquisition and synthesis, the generalized Wang tile set presented in this part of the talk allows us to seamlessly and non-periodically tile texture data on parameterized surfaces of arbitrary topology. Once we synthesize textures on tiles, we can reuse the same tile set on different surfaces and we can also instantaneously change the surface appearance by just switching the reference tile set. Further than color textures, we also extend surface tiling to include bump maps, geometry details, the BTF's, as well as Poisson disk tiling. The lecturer for the third part is Chi-Wing Fu, who wrote several papers on this topic [Fu and Leung, 2005].Non-Periodic Tilings for Computer Graphics Applications The fourth part of the class covers an important class of non-periodic tilings and their benefits for computer graphics applications. First, the theory of Penrose tilings is presented. We show how the inherent self-similarity of Penrose tiling can be exploited in order to get efficient implementation of uniform distributions with blue-noise properties. Then, we present polyomino-based uniform distributions, and show their advantages. Finally, we explore other non-periodic tiling systems, potentially usable for computer graphics applications: dodecagonal tiling, Ammann tiling, etc. The lecturer for the fourth part is Victor Ostromoukhov who is an expert in this topic [Ostromoukhov et al., 2004; Ostromoukhov, 2007].Tile-Based Methods for Non-Photorealistic Rendering and Landscape Modeling The fifth part of the class covers applications of tile-based methods in the fields of non-photorealistic rendering and landscape modeling [Cohen et al., 2003]. Using hierarchical tile sets one is able to create point sets with infinite density still showing Poisson disk characteristics [Kopf et al., 2006]. We will demonstrate this using a set of tiles that is recursively subdivided. This is possible because the set shows self similarity. The resulting points can be used to create stipple drawings and also distributions of plants that also show Poisson disk behavior. This will be demonstrated by an application that enables real-time modeling and rendering of complex landscapes. The lecturer for the fifth part is Oliver Deussen, who has considerable experience with tile-based design.},
booktitle = {ACM SIGGRAPH 2008 Classes},
articleno = {93},
numpages = {267},
location = {Los Angeles, California},
series = {SIGGRAPH '08}
}

@article{Yuksel2019,
   author       = {Cem Yuksel and Sylvain Lefebvre and Marco Tarini},
   title        = {Rethinking Texture Mapping},
   journal      = {Computer Graphics Forum (Proceedings of Eurographics 2019)},
   year         = {2019},
   volume       = {38},
   number       = {2},
   pages        = {535--551},
   numpages     = {17},
   location     = {Genova, Italy},
   url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13656},
   doi          = {10.1111/cgf.13656},
}

@article{Rodriguez_Pardo_2023,
   title={SeamlessGAN: Self-Supervised Synthesis of Tileable Texture Maps},
   volume={29},
   ISSN={2160-9306},
   url={http://dx.doi.org/10.1109/TVCG.2022.3143615},
   DOI={10.1109/tvcg.2022.3143615},
   number={6},
   journal={IEEE Transactions on Visualization and Computer Graphics},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Rodriguez, Carlos and Garces, Elena},
   year={2023},
   month=jun, pages={2914–2925} }

@misc{substance_sampler,
    title        = {Substance 3D Sampler},
    author       = {Adobe},
    year         = {2023},
    note         ={\url{https://www.adobe.com/products/substance3d-sampler.html} [Accessed: Jan 2024]}}

@INPROCEEDINGS{efros99,
  author={Efros, A.A. and Leung, T.K.},
  booktitle={Proceedings of the Seventh IEEE International Conference on Computer Vision}, 
  title={Texture synthesis by non-parametric sampling}, 
  year={1999},
  volume={2},
  number={},
  pages={1033-1038 vol.2},
  doi={10.1109/ICCV.1999.790383}}

@article{blinn76,
author = {Blinn, James F. and Newell, Martin E.},
title = {Texture and Reflection in Computer Generated Images},
year = {1976},
issue_date = {Oct. 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/360349.360353},
doi = {10.1145/360349.360353},
journal = {Commun. ACM},
month = {oct},
pages = {542–547},
numpages = {6} }

@article{10.1145/97880.97902,
author = {Haeberli, Paul},
title = {Paint by Numbers: Abstract Image Representations},
year = {1990},
issue_date = {Aug. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0097-8930},
url = {https://doi.org/10.1145/97880.97902},
doi = {10.1145/97880.97902},
abstract = {Computer graphics research has concentrated on creating photo-realistic images of synthetic objects. These images communicate surface shading and curvature, as well as the depth relationships of objects in a scene. These renderings are traditionally represented by a rectangular array of pixels that tile the image plane.As an alternative to photo-realism, it is possible to create abstract images using an ordered collection of brush strokes. These abstract images filter and refine visual information before it is presented to the viewer. By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.},
journal = {SIGGRAPH Comput. Graph.},
month = {sep},
pages = {207–214},
numpages = {8}
}

@inproceedings{haeberli90,
author = {Haeberli, Paul},
title = {Paint by Numbers: Abstract Image Representations},
year = {1990},
isbn = {0897913442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/97879.97902},
doi = {10.1145/97879.97902},
abstract = {Computer graphics research has concentrated on creating photo-realistic images of synthetic objects. These images communicate surface shading and curvature, as well as the depth relationships of objects in a scene. These renderings are traditionally represented by a rectangular array of pixels that tile the image plane.As an alternative to photo-realism, it is possible to create abstract images using an ordered collection of brush strokes. These abstract images filter and refine visual information before it is presented to the viewer. By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.},
booktitle = {Proceedings of the 17th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {207–214},
numpages = {8},
location = {Dallas, TX, USA},
series = {SIGGRAPH '90}
}

@inproceedings{novello2023neural,
  title={Neural Implicit Surface Evolution},
  author={Novello, Tiago and Da Silva, Vinicius and Schardong, Guilherme and Schirmer, Luiz and Lopes, Helio and Velho, Luiz},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={14279--14289},
  year={2023}
}


%%%  LVELHO 2024.03.17

%%%%

@inproceedings{superres-2018,
  title={360 Panorama Super-resolution using Deep Convolutional Networks},
  author={Vida Fakour Sevom and Esin Guldogan and J. K{\"a}m{\"a}r{\"a}inen},
  booktitle={VISIGRAPP},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:4394895}
}

%%%

@inproceedings{omninerf-2022,
  TITLE = {{Omni-nerf: neural radiance field from 360{\textdegree} image captures}},
  AUTHOR = {Gu, Kai and Maugey, Thomas and Knorr, Sebastian and Guillemot, Christine},
  URL = {https://inria.hal.science/hal-03646688},
  BOOKTITLE = {{ICME 2022 - IEEE international conference on multimedia and expo}},
  ADDRESS = {Taipei, Taiwan},
  PUBLISHER = {{IEEE}},
  PAGES = {1-6},
  YEAR = {2022},
  MONTH = Jul,
  KEYWORDS = {view synthesis ; rendering ; omni-directional imaging ; Light field ; deep learning},
  PDF = {https://inria.hal.science/hal-03646688/file/Omni_NeRF__Neural_Radiance_Field_from_360__image_captures_camera_ready.pdf},
  HAL_ID = {hal-03646688},
  HAL_VERSION = {v1},
}

%%%

@misc{360fusion-2022,
      title={360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance}, 
      author={Shreyas Kulkarni and Peng Yin and Sebastian Scherer},
      year={2022},
      eprint={2209.14265},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{pano-nn-2023,
author = {Khamiyev, Izat and Issa, Dias and Akhtar, Zahid and Demirci, M. Fatih},
title = {Panoramic image generation using deep neural networks},
year = {2023},
issue_date = {Jul 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-023-08056-5},
doi = {10.1007/s00500-023-08056-5},
abstract = {A traditional approach for panoramic image generation consists of a random sample consensus (RANSAC) algorithm on a set of scale-invariant feature transform (SIFT) correspondences to generate a homography matrix between two images. Although producing adequate results for some type of images, hand-crafted SIFT features are not robust enough for highly varying natural images and the iterative RANSAC algorithm with its randomness does not always find the desired homography matrix. Recently, deep neural networks have been producing significant results in many challenging computer vision problems by learning features from large amounts of data. However, only very few recent works have been applied deep learning to panoramic image generation with the objective of finding feature correspondences and estimating homography matrix. Moreover, the absence of a proper dataset for the image stitching task hinders the standardization of models and comparison of their results. This paper attempts to generate panoramic images by extensively experimenting with various approaches using deep neural networks. The best proposed deep learning model achieved 7.31 and 1.07 pixels of the average absolute value loss for corner difference in X and Y directions, respectively. At the same time, qualitative results demonstrate superiority in comparison with the state-of-the-art SIFT+RANSAC algorithm. Specifically, in 72\% of time the proposed framework either produced better results than SIFT+RANSAC or results of the proposed approach and SIFT+RANSAC were indistinguishable. Although SIFT + RANSAC produces better results in 28\% of the time with respect to the loss function, our results are still visually comparable in many of these cases. Finally, a novel panoramic image generation dataset is introduced in this paper.},
journal = {Soft Comput.},
month = {apr},
pages = {8679–8695},
numpages = {17},
keywords = {Computer vision, Convolutional neural networks, Homography matrix, Image stitching}
}

%%

@misc{perf-2023,
      title={PERF: Panoramic Neural Radiance Field from a Single Panorama}, 
      author={Guangcong Wang and Peng Wang and Zhaoxi Chen and Wenping Wang and Chen Change Loy and Ziwei Liu},
      year={2023},
      eprint={2310.16831},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


%% Barron

@article{barron2022mipnerf360,
    title={Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields},
    author={Jonathan T. Barron and Ben Mildenhall and 
            Dor Verbin and Pratul P. Srinivasan and Peter Hedman},
    journal={CVPR},
    year={2022}
}

%% Hugues

@article{grad-sphere-2010,
author = {Kazhdan, Michael and Surendran, Dinoj and Hoppe, Hugues},
title = {Distributed gradient-domain processing of planar and spherical images},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/1731047.1731052},
doi = {10.1145/1731047.1731052},
abstract = {Gradient-domain processing is widely used to edit and combine images. In this article we extend the framework in two directions. First, we adapt the gradient-domain approach to operate on a spherical domain, to enable operations such as seamless stitching, dynamic-range compression, and gradient-based sharpening over spherical imagery. An efficient streaming computation is obtained using a new spherical parameterization with bounded distortion and localized boundary constraints. Second, we design a distributed solver to efficiently process large planar or spherical images. The solver partitions images into bands, streams through these bands in parallel within a networked cluster, and schedules computation to hide the necessary synchronization latency. We demonstrate our contributions on several datasets including the Digitized Sky Survey, a terapixel spherical scan of the night sky.},
journal = {ACM Trans. Graph.},
month = {apr},
articleno = {14},
numpages = {11},
keywords = {Panoramas, distributed solver, screened Poisson equation, spherical parameterization, streaming multigrid}
}

@article{sphere-mr-2010,
author = {Kazhdan, Michael and Hoppe, Hugues},
title = {Metric-aware processing of spherical imagery},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/1882261.1866175},
doi = {10.1145/1882261.1866175},
abstract = {Processing spherical images is challenging. Because no spherical parameterization is globally uniform, an accurate solver must account for the spatially varying metric. We present the first efficient metric-aware solver for Laplacian processing of spherical data. Our approach builds on the commonly used equirectangular parameterization, which provides differentiability, axial symmetry, and grid sampling. Crucially, axial symmetry lets us discretize the Laplacian operator just once per grid row. One difficulty is that anisotropy near the poles leads to a poorly conditioned system. Our solution is to construct an adapted hierarchy of finite elements, adjusted at the poles to maintain derivative continuity, and selectively coarsened to bound element anisotropy. The resulting elements are nested both within and across resolution levels. A streaming multigrid solver over this hierarchy achieves excellent convergence rate and scales to huge images. We demonstrate applications in reaction-diffusion texture synthesis and panorama stitching and sharpening.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {149},
numpages = {10},
keywords = {Laplace-Beltrami, equirectangular, panoramas}
}

@article{panovid-loop-2017,
author = {He, Mingming and Liao, Jing and Sander, Pedro V. and Hoppe, Hugues},
title = {Gigapixel Panorama Video Loops},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3144455},
doi = {10.1145/3144455},
abstract = {We present the first technique to create wide-angle, high-resolution looping panoramic videos. Starting with a 2D grid of registered videos acquired on a robotic mount, we formulate a combinatorial optimization to determine for each output pixel the source video and looping parameters that jointly maximize spatiotemporal consistency. This optimization is accelerated by reducing the set of source labels using a graph-coloring scheme. We parallelize the computation and implement it out-of-core by partitioning the domain along low-importance paths. The merged panorama is assembled using gradient-domain blending and stored as a hierarchy of video tiles. Finally, an interactive viewer adaptively preloads these tiles for responsive browsing and allows the user to interactively edit and improve local regions. We demonstrate these techniques on gigapixel-sized looping panoramas.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {3},
numpages = {15},
keywords = {Video textures, cinemagraphs, video stitching}
}

% Szelisky

@article{pano-mosaic-2000,
  title={Systems and Experiment Paper: Construction of Panoramic Image Mosaics with Global and Local Alignment},
  author={Harry Shum and Richard Szeliski},
  journal={International Journal of Computer Vision},
  year={2000},
  volume={36},
  pages={101-130},
  url={https://api.semanticscholar.org/CorpusID:3364988}
}

@article{pano-vid-2005,
author = {Agarwala, Aseem and Zheng, Ke Colin and Pal, Chris and Agrawala, Maneesh and Cohen, Michael and Curless, Brian and Salesin, David and Szeliski, Richard},
title = {Panoramic video textures},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1073204.1073268},
doi = {10.1145/1073204.1073268},
abstract = {This paper describes a mostly automatic method for taking the output of a single panning video camera and creating a panoramic video texture (PVT): a video that has been stitched into a single, wide field of view and that appears to play continuously and indefinitely. The key problem in creating a PVT is that although only a portion of the scene has been imaged at any given time, the output must simultaneously portray motion throughout the scene. Like previous work in video textures, our method employs min-cut optimization to select fragments of video that can be stitched together both spatially and temporally. However, it differs from earlier work in that the optimization must take place over a much larger set of data. Thus, to create PVTs, we introduce a dynamic programming step, followed by a novel hierarchical min-cut optimization algorithm. We also use gradient-domain compositing to further smooth boundaries between video fragments. We demonstrate our results with an interactive viewer in which users can interactively pan and zoom on high-resolution PVTs.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {821–827},
numpages = {7},
keywords = {panoramas, video textures, video-based rendering}
}

@inproceedings{layer-pano-2020,
author = {Lin, Kai-En and Xu, Zexiang and Mildenhall, Ben and Srinivasan, Pratul P. and Hold-Geoffroy, Yannick and DiVerdi, Stephen and Sun, Qi and Sunkavalli, Kalyan and Ramamoorthi, Ravi},
title = {Deep Multi Depth Panoramas for View Synthesis},
year = {2020},
isbn = {978-3-030-58600-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58601-0_20},
doi = {10.1007/978-3-030-58601-0_20},
abstract = {We propose a learning-based approach for novel view synthesis for multi-camera 360∘ panorama capture rigs. Previous work constructs RGBD panoramas from such data, allowing for view synthesis with small amounts of translation, but cannot handle the disocclusions and view-dependent effects that are caused by large translations. To address this issue, we present a novel scene representation—Multi Depth Panorama (MDP)—that consists of multiple RGBDα panoramas that represent both scene geometry and appearance. We demonstrate a deep neural network-based method to reconstruct MDPs from multi-camera 360∘ images. MDPs are more compact than previous 3D scene representations and enable high-quality, efficient new view rendering. We demonstrate this via experiments on both synthetic and real data and comparisons with previous state-of-the-art methods spanning both learning-based approaches and classical RGBD-based methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII},
pages = {328–344},
numpages = {17},
keywords = {360∘ panoramas, View synthesis, Image-based rendering, Virtual reality},
location = {Glasgow, United Kingdom}
}

@article{pano-capt-2017,
author = {Matzen, Kevin and Cohen, Michael F. and Evans, Bryce and Kopf, Johannes and Szeliski, Richard},
title = {Low-cost 360 stereo photography and video capture},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073645},
doi = {10.1145/3072959.3073645},
abstract = {A number of consumer-grade spherical cameras have recently appeared, enabling affordable monoscopic VR content creation in the form of full 360° X 180° spherical panoramic photos and videos. While monoscopic content is certainly engaging, it fails to leverage a main aspect of VR HMDs, namely stereoscopic display. Recent stereoscopic capture rigs involve placing many cameras in a ring and synthesizing an omni-directional stereo panorama enabling a user to look around to explore the scene in stereo. In this work, we describe a method that takes images from two 360° spherical cameras and synthesizes an omni-directional stereo panorama with stereo in all directions. Our proposed method has a lower equipment cost than camera-ring alternatives, can be assembled with currently available off-the-shelf equipment, and is relatively small and light-weight compared to the alternatives. We validate our method by generating both stills and videos. We have conducted a user study to better understand what kinds of geometric processing are necessary for a pleasant viewing experience. We also discuss several algorithmic variations, each with their own time and quality trade-offs.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {148},
numpages = {12},
keywords = {virtual reality, stereo, panoramas, image stitching}
}


%% Lowe

@article{sift-pano-2007,
author = {Brown, Matthew and Lowe, David G.},
title = {Automatic Panoramic Image Stitching using Invariant Features},
year = {2007},
issue_date = {Aug 2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-006-0002-3},
doi = {10.1007/s11263-006-0002-3},
abstract = {This paper concerns the problem of fully automated panoramic image stitching. Though the 1D problem (single axis of rotation) is well studied, 2D or multi-row stitching is more difficult. Previous approaches have used human input or restrictions on the image sequence in order to establish matching images. In this work, we formulate stitching as a multi-image matching problem, and use invariant local features to find matches between all of the images. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the input images. It is also insensitive to noise images that are not part of a panorama, and can recognise multiple panoramas in an unordered image dataset. In addition to providing more detail, this paper extends our previous work in the area (Brown and Lowe, 2003) by introducing gain compensation and automatic straightening steps.},
journal = {Int. J. Comput. Vision},
month = {aug},
pages = {59–73},
numpages = {15},
keywords = {recognition, stitching, multi-image matching}
}

%% Apple

@inproceedings{qt-vr-1995,
author = {Chen, Shenchang Eric},
title = {QuickTime VR: an image-based approach to virtual environment navigation},
year = {1995},
isbn = {0897917014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/218380.218395},
doi = {10.1145/218380.218395},
booktitle = {Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques},
pages = {29–38},
numpages = {10},
keywords = {virtual reality, view interpolation, real-time display, panoramic images, image warping, image registration, environment maps},
series = {SIGGRAPH '95}
}

%%

@inproceedings{2020nerf,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}

@inproceedings{ sib22-dvox,
  author = "Daniel Perazzo and Joao Paulo Lima and Luiz Velho and Veronica Teichrieb",
  booktitle = "Proceedings of SIBGRAPI",
  title = "DirectVoxGO++: Fast Neural Radiance Fields for Object Reconstruction",
  year = "2022",
}

@article{ravi2020pytorch3d,
    author = {Nikhila Ravi and Jeremy Reizenstein and David Novotny and Taylor Gordon
                  and Wan-Yen Lo and Justin Johnson and Georgia Gkioxari},
    title = {Accelerating 3D Deep Learning with PyTorch3D},
    journal = {arXiv:2007.08501},
    year = {2020},
}